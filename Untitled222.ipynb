{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNsbfEeOmUfIU1CnhRkkRG7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nvinogradskaya/DL_HW4_RNN/blob/main/Untitled222.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pCVDzt9K_PRV"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tqdm.auto import tqdm\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding, Concatenate, LayerNormalization, Attention\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from geopy.distance import geodesic\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_USERS = 170\n",
        "SEQ_LENGTH = 10\n",
        "LSTM_UNITS = 128\n",
        "BATCH_SIZE = 128\n",
        "EPOCHS = 20\n",
        "TEST_SIZE = 0.3\n",
        "EMBEDDING_DIM = 32"
      ],
      "metadata": {
        "id": "eaAKLqFG_l5b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data_with_stay_points(data_path):\n",
        "    data = []\n",
        "    user_map = {}\n",
        "    user_dirs = sorted(os.listdir(data_path))[:MAX_USERS]\n",
        "\n",
        "    for idx, user in enumerate(tqdm(user_dirs, desc=\"Users\")):\n",
        "        user_map[user] = idx\n",
        "        traj_dir = os.path.join(data_path, user, 'Trajectory')\n",
        "\n",
        "        # Параллельная обработка файлов\n",
        "        plt_files = [f for f in os.listdir(traj_dir) if f.endswith('.plt')]\n",
        "        for file in plt_files:\n",
        "            df = pd.read_csv(\n",
        "                os.path.join(traj_dir, file),\n",
        "                skiprows=6,\n",
        "                header=None,\n",
        "                usecols=[0, 1, 3, 5, 6],\n",
        "                names=['lat', 'lon', 'alt', 'date', 'time']\n",
        "            )\n",
        "            # Ускорение обработки datetime\n",
        "            df['datetime'] = pd.to_datetime(df['date'] + ' ' + df['time'], errors='coerce')\n",
        "            df.dropna(subset=['datetime'], inplace=True)\n",
        "            df['user_id'] = idx\n",
        "\n",
        "            # Обновленные параметры stay points\n",
        "            stay_df = detect_stay_points(df, dist_thresh=50, time_thresh=600)\n",
        "            if stay_df is not None and len(stay_df) > SEQ_LENGTH:\n",
        "                data.append(stay_df)\n",
        "\n",
        "    if not data:\n",
        "        raise ValueError(\"No valid data after processing\")\n",
        "\n",
        "    df_all = pd.concat(data, ignore_index=True)\n",
        "\n",
        "    # Исправленная фильтрация\n",
        "    df_all = df_all[\n",
        "        (df_all.lat.between(-90, 90)) &\n",
        "        (df_all.lon.between(-180, 180)) &\n",
        "        ~((df_all.lat == 0) & (df_all.lon == 0))\n",
        "    ].dropna()\n",
        "\n",
        "    # Корректная нормализация\n",
        "    scaler = MinMaxScaler()\n",
        "    df_all[['lat', 'lon', 'alt']] = scaler.fit_transform(df_all[['lat', 'lon', 'alt']])\n",
        "\n",
        "    # Отладочная информация\n",
        "    print(f\"\\nLoaded {len(df_all)} points from {len(user_map)} users\")\n",
        "    print(f\"Average points per user: {len(df_all)/len(user_map):.1f}\")\n",
        "\n",
        "    return df_all, user_map, scaler"
      ],
      "metadata": {
        "id": "mfeLkuA4_rsb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_stay_points(df, dist_thresh=50, time_thresh=600):\n",
        "    stays = []\n",
        "    i = 0\n",
        "    while i < len(df)-1:\n",
        "        j = i+1\n",
        "        valid_points = 0\n",
        "\n",
        "        while j < len(df):\n",
        "            try:\n",
        "                d = geodesic((df.iloc[i].lat, df.iloc[i].lon),\n",
        "                            (df.iloc[j].lat, df.iloc[j].lon)).meters\n",
        "                t = (df.iloc[j].datetime - df.iloc[i].datetime).total_seconds()\n",
        "\n",
        "                if d > dist_thresh or t > time_thresh:\n",
        "                    if valid_points >= 3:  # Минимум 3 точки для кластера\n",
        "                        stays.append({\n",
        "                            'lat': df.iloc[i:j].lat.mean(),\n",
        "                            'lon': df.iloc[i:j].lon.mean(),\n",
        "                            'alt': df.iloc[i:j].alt.mean(),\n",
        "                            'datetime': df.iloc[i].datetime,\n",
        "                            'user_id': df.iloc[i].user_id\n",
        "                        })\n",
        "                    break\n",
        "                valid_points += 1\n",
        "                j += 1\n",
        "            except:\n",
        "                j += 1\n",
        "        i = j if j > i else i+1\n",
        "\n",
        "    return pd.DataFrame(stays) if stays else None"
      ],
      "metadata": {
        "id": "SyTg6JaI_ur9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Обновленная функция create_sequences\n",
        "def create_sequences(df, seq_length=SEQ_LENGTH, step=1, test_size=TEST_SIZE):\n",
        "    X_train, X_test = [], []\n",
        "    y_train, y_test = [], []\n",
        "    u_train, u_test = [], []\n",
        "    t_train, t_test = [], []\n",
        "\n",
        "    df = df.sort_values(['user_id', 'datetime'])\n",
        "    for uid in tqdm(df.user_id.unique(), desc=\"Creating sequences\"):\n",
        "        user_df = df[df.user_id == uid]\n",
        "        if len(user_df) < seq_length + 1:\n",
        "            continue\n",
        "\n",
        "        # Разделение траекторий пользователя\n",
        "        split_idx = int(len(user_df) * (1 - test_size))\n",
        "        train_user = user_df.iloc[:split_idx]\n",
        "        test_user = user_df.iloc[split_idx:]\n",
        "\n",
        "        # Генерация последовательностей для train\n",
        "        for i in range(0, len(train_user) - seq_length, step):\n",
        "            seq = train_user.iloc[i:i+seq_length]\n",
        "            target = train_user.iloc[i+seq_length]\n",
        "            X_train.append(seq[['lat', 'lon', 'alt']].values)\n",
        "            y_train.append(target[['lat', 'lon']].values)\n",
        "            u_train.append(uid)\n",
        "            t_train.append(target['datetime'].hour)\n",
        "\n",
        "        # Генерация последовательностей для test\n",
        "        for i in range(0, len(test_user) - seq_length, step):\n",
        "            seq = test_user.iloc[i:i+seq_length]\n",
        "            target = test_user.iloc[i+seq_length]\n",
        "            X_test.append(seq[['lat', 'lon', 'alt']].values)\n",
        "            y_test.append(target[['lat', 'lon']].values)\n",
        "            u_test.append(uid)\n",
        "            t_test.append(target['datetime'].hour)\n",
        "\n",
        "    # Конвертация в numpy arrays\n",
        "    X_train = np.array(X_train)\n",
        "    X_test = np.array(X_test)\n",
        "    y_train = np.array(y_train)\n",
        "    y_test = np.array(y_test)\n",
        "    u_train = np.array(u_train)\n",
        "    u_test = np.array(u_test)\n",
        "    t_train = np.array(t_train)\n",
        "    t_test = np.array(t_test)\n",
        "\n",
        "    print(f\"\\nTrain sequences: {len(X_train)} | Test sequences: {len(X_test)}\")\n",
        "    return X_train, X_test, y_train, y_test, u_train, u_test, t_train, t_test"
      ],
      "metadata": {
        "id": "ri-rwLpl_xkl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Lambda\n",
        "\n",
        "from tensorflow.keras.layers import Bidirectional, MultiHeadAttention\n",
        "\n",
        "# Новые параметры\n",
        "LSTM_UNITS = 256  # Увеличили размерность\n",
        "ATTN_HEADS = 4    # Количество голов внимания\n",
        "KEY_DIM = 64      # Размерность ключей/запросов\n",
        "\n",
        "from tensorflow.keras.layers import Lambda\n",
        "from tensorflow.keras.layers import Lambda, GlobalAveragePooling1D\n",
        "\n",
        "def build_deepmove_model():\n",
        "    # Входные слои\n",
        "    coord_input = Input(shape=(SEQ_LENGTH, 3), name='coord_input')\n",
        "    user_input = Input(shape=(), dtype='int32', name='user_input')\n",
        "    time_input = Input(shape=(), dtype='int32', name='time_input')\n",
        "\n",
        "    # Эмбеддинги контекста\n",
        "    user_emb = Embedding(MAX_USERS, EMBEDDING_DIM)(user_input)\n",
        "    time_emb = Embedding(24, EMBEDDING_DIM)(time_input)\n",
        "\n",
        "    # 1. Двунаправленный LSTM\n",
        "    lstm_out = Bidirectional(\n",
        "        LSTM(LSTM_UNITS, return_sequences=True),\n",
        "        merge_mode='concat'\n",
        "    )(coord_input)\n",
        "\n",
        "    # 2. Иерархическое внимание\n",
        "    spatial_attention = MultiHeadAttention(\n",
        "        num_heads=ATTN_HEADS,\n",
        "        key_dim=KEY_DIM\n",
        "    )(lstm_out, lstm_out)\n",
        "\n",
        "    temporal_attention = MultiHeadAttention(\n",
        "        num_heads=ATTN_HEADS,\n",
        "        key_dim=KEY_DIM\n",
        "    )(spatial_attention, spatial_attention)\n",
        "\n",
        "    # 3. Объединение с контекстом\n",
        "    context_concat = Concatenate()([user_emb, time_emb])\n",
        "    context = Lambda(lambda x: tf.expand_dims(x, axis=1))(context_concat)  # Добавление оси\n",
        "\n",
        "    context_attention = MultiHeadAttention(\n",
        "        num_heads=ATTN_HEADS//2,\n",
        "        key_dim=KEY_DIM\n",
        "    )(temporal_attention, context)\n",
        "\n",
        "    # 4. Аггрегация и выход (исправлено)\n",
        "    attn_flat = Lambda(lambda x: tf.reduce_mean(x, axis=1))(context_attention)  # Обертка для reduce_mean\n",
        "    output = Dense(2, activation='linear')(attn_flat)\n",
        "\n",
        "    model = Model(\n",
        "        inputs=[coord_input, user_input, time_input],\n",
        "        outputs=output\n",
        "    )\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(0.001),\n",
        "        loss='mse',\n",
        "        metrics=['mae']\n",
        "    )\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "8jOor1n7_1rf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "data_path = \"/content/drive/My Drive/Colab Notebooks/Data/\""
      ],
      "metadata": {
        "id": "SpxaHaY1_7t7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df, user_map, scaler = load_data_with_stay_points(data_path)"
      ],
      "metadata": {
        "id": "mtIoQdmrAG7v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test, u_train, u_test, t_train, t_test = create_sequences(df)"
      ],
      "metadata": {
        "id": "dxp7BNHrAQ71"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Input, Bidirectional, LSTM, MultiHeadAttention, Concatenate, Embedding, Dense, Lambda\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "\n",
        "# Параметры модели (оставлены без изменений)\n",
        "LSTM_UNITS = 256\n",
        "ATTN_HEADS = 4\n",
        "KEY_DIM = 64\n",
        "EMBEDDING_DIM = 32\n",
        "\n",
        "def build_deepmove_model():\n",
        "    # Входные слои (без изменений)\n",
        "    coord_input = Input(shape=(SEQ_LENGTH, 3), name='coord_input')\n",
        "    user_input = Input(shape=(), dtype='int32', name='user_input')\n",
        "    time_input = Input(shape=(), dtype='int32', name='time_input')\n",
        "\n",
        "    # Эмбеддинги (без изменений)\n",
        "    user_emb = Embedding(MAX_USERS, EMBEDDING_DIM)(user_input)\n",
        "    time_emb = Embedding(24, EMBEDDING_DIM)(time_input)\n",
        "\n",
        "    # LSTM + Attention блоки (без изменений архитектуры)\n",
        "    lstm_out = Bidirectional(\n",
        "        LSTM(LSTM_UNITS, return_sequences=True),\n",
        "        merge_mode='concat'\n",
        "    )(coord_input)\n",
        "\n",
        "    spatial_attention = MultiHeadAttention(\n",
        "        num_heads=ATTN_HEADS,\n",
        "        key_dim=KEY_DIM\n",
        "    )(lstm_out, lstm_out)\n",
        "\n",
        "    temporal_attention = MultiHeadAttention(\n",
        "        num_heads=ATTN_HEADS,\n",
        "        key_dim=KEY_DIM\n",
        "    )(spatial_attention, spatial_attention)\n",
        "\n",
        "    context_concat = Concatenate()([user_emb, time_emb])\n",
        "    context = Lambda(lambda x: tf.expand_dims(x, axis=1))(context_concat)\n",
        "\n",
        "    context_attention = MultiHeadAttention(\n",
        "        num_heads=ATTN_HEADS//2,\n",
        "        key_dim=KEY_DIM\n",
        "    )(temporal_attention, context)\n",
        "\n",
        "    attn_flat = Lambda(lambda x: tf.reduce_mean(x, axis=1))(context_attention)\n",
        "    output = Dense(2, activation='linear')(attn_flat)\n",
        "\n",
        "    # Создание модели\n",
        "    model = Model(\n",
        "        inputs=[coord_input, user_input, time_input],\n",
        "        outputs=output\n",
        "    )\n",
        "\n",
        "    # Ключевые изменения в настройке обучения:\n",
        "    optimizer = Adam(\n",
        "        learning_rate=0.0001,  # Уменьшенный learning rate\n",
        "        beta_1=0.9,\n",
        "        beta_2=0.999,\n",
        "        epsilon=1e-07\n",
        "    )\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss='mse',\n",
        "        metrics=['mae']\n",
        "    )\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "t7xom-fVd6xQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_deepmove_model()\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "0fZXYp9LWBQp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = X_train.astype(np.float32)\n",
        "y_train = y_train.astype(np.float32)\n",
        "u_train = u_train.astype(np.int32)\n",
        "t_train = t_train.astype(np.int32)\n",
        "\n",
        "X_test = X_test.astype(np.float32)\n",
        "y_test = y_test.astype(np.float32)\n",
        "u_test = u_test.astype(np.int32)\n",
        "t_test = t_test.astype(np.int32)\n"
      ],
      "metadata": {
        "id": "mN0bU1DoCiTI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "SAVE_PATH = \"/content/drive/My Drive/deepmove_results-v5/\"\n",
        "os.makedirs(SAVE_PATH, exist_ok=True)\n",
        "\n",
        "checkpoint_path = os.path.join(SAVE_PATH, \"best_model.h5\")\n",
        "\n",
        "callbacks = [\n",
        "    EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n",
        "    ModelCheckpoint(filepath=checkpoint_path, monitor='val_loss', save_best_only=True, verbose=1)\n",
        "]"
      ],
      "metadata": {
        "id": "tv5-QgqEFWfU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Добавляем планировщик обучения\n",
        "lr_scheduler = ReduceLROnPlateau(\n",
        "    monitor='val_loss',\n",
        "    factor=0.5,\n",
        "    patience=2,\n",
        "    verbose=1,\n",
        "    min_lr=1e-6\n",
        ")\n",
        "\n",
        "# Обновленный вызов fit с shuffle=True\n",
        "history = model.fit(\n",
        "    {'coord_input': X_train, 'user_input': u_train, 'time_input': t_train},\n",
        "    y_train,\n",
        "    validation_split=0.1,\n",
        "    epochs=EPOCHS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    callbacks=[callbacks, lr_scheduler],  # Добавляем планировщик\n",
        "    shuffle=True  # Ключевое добавление!\n",
        ")"
      ],
      "metadata": {
        "id": "34cFJo5Q1bT1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_weights(checkpoint_path)"
      ],
      "metadata": {
        "id": "Ee13CSEaFlVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_training_history(history):\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['loss'], label='Train Loss')\n",
        "    plt.plot(history.history['val_loss'], label='Val Loss')\n",
        "    plt.title('Loss over Epochs')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('MSE Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['mae'], label='Train MAE')\n",
        "    plt.plot(history.history['val_mae'], label='Val MAE')\n",
        "    plt.title('MAE over Epochs')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('MAE')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_training_history(history)"
      ],
      "metadata": {
        "id": "HTA0FFZ_WEY6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from geopy.distance import geodesic\n",
        "import numpy as np\n",
        "\n",
        "def evaluate_metrics(y_true, y_pred, users_test, scaler=None):\n",
        "    # Если данные масштабированы, используем обратное преобразование\n",
        "    if scaler is not None:\n",
        "        dummy_alt = np.zeros((len(y_true), 1))\n",
        "        y_true = scaler.inverse_transform(np.hstack([y_true, dummy_alt]))[:, :2]\n",
        "        y_pred = scaler.inverse_transform(np.hstack([y_pred, dummy_alt]))[:, :2]\n",
        "\n",
        "    # Расчет ошибок в метрах\n",
        "    errors_meters = np.array([\n",
        "        geodesic((true_lat, true_lon), (pred_lat, pred_lon)).meters\n",
        "        for (true_lat, true_lon), (pred_lat, pred_lon)\n",
        "        in zip(y_true, y_pred)\n",
        "    ])\n",
        "\n",
        "    # Разделение ошибок по координатам\n",
        "    lat_errors = np.abs(y_true[:, 0] - y_pred[:, 0])\n",
        "    lon_errors = np.abs(y_true[:, 1] - y_pred[:, 1])\n",
        "\n",
        "    # Расчет FDE для последних точек пользователей\n",
        "    last_indices = []\n",
        "    for uid in np.unique(users_test):\n",
        "        user_indices = np.where(users_test == uid)[0]\n",
        "        if user_indices.size > 0:\n",
        "            last_indices.append(user_indices[-1])\n",
        "    fde = np.mean(errors_meters[last_indices])\n",
        "\n",
        "    # Все метрики\n",
        "    metrics = {\n",
        "        'ADE (m)': np.mean(errors_meters),\n",
        "        'FDE (m)': fde,\n",
        "        'Median Error (m)': np.median(errors_meters),\n",
        "        '95th Percentile (m)': np.percentile(errors_meters, 95),\n",
        "        'RMSE (m)': np.sqrt(np.mean(errors_meters**2)),\n",
        "        'Within-50m (%)': np.mean(errors_meters < 50) * 100,\n",
        "        'Within-100m (%)': np.mean(errors_meters < 100) * 100,\n",
        "        'Max Error (m)': np.max(errors_meters),\n",
        "        'Lat MAE (deg)': np.mean(lat_errors),\n",
        "        'Lon MAE (deg)': np.mean(lon_errors)\n",
        "    }\n",
        "\n",
        "    # Красивый вывод\n",
        "    print(\"\\n=== Evaluation Metrics ===\")\n",
        "    for name, value in metrics.items():\n",
        "        if '%' in name:\n",
        "            print(f\"{name}: {value:.2f}%\")\n",
        "        elif 'deg' in name:\n",
        "            print(f\"{name}: {value:.6f}°\")\n",
        "        else:\n",
        "            print(f\"{name}: {value:.2f}\")\n",
        "\n",
        "    return metrics"
      ],
      "metadata": {
        "id": "n1QtqMlRGe6t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "with tqdm(total=len(X_test), desc=\"Prediction Progress\") as pbar:\n",
        "    y_pred = model.predict({'coord_input': X_test, 'user_input': u_test, 'time_input': t_test})\n",
        "    pbar.update(len(X_test))"
      ],
      "metadata": {
        "id": "kHTBWgLOGt9R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics = evaluate_metrics(\n",
        "    y_true=y_test,\n",
        "    y_pred=y_pred,\n",
        "    users_test=u_test,\n",
        "    scaler=scaler  # Передаем scaler для авто-денормализации\n",
        ")"
      ],
      "metadata": {
        "id": "9zxA8qGYGfwu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_metrics(y_true, y_pred, users_test, scaler=None):\n",
        "    if scaler is not None:\n",
        "        # Добавим фиктивную alt = 0, чтобы соответствовать 3 признакам\n",
        "        y_true_3d = np.hstack([y_true, np.zeros((len(y_true), 1))])\n",
        "        y_pred_3d = np.hstack([y_pred, np.zeros((len(y_pred), 1))])\n",
        "\n",
        "        y_true = scaler.inverse_transform(y_true_3d)[:, :2]\n",
        "        y_pred = scaler.inverse_transform(y_pred_3d)[:, :2]\n",
        "\n",
        "    # Остальная часть — без изменений\n",
        "    errors_meters = np.array([\n",
        "        geodesic((true_lat, true_lon), (pred_lat, pred_lon)).meters\n",
        "        for (true_lat, true_lon), (pred_lat, pred_lon)\n",
        "        in zip(y_true, y_pred)\n",
        "    ])\n",
        "\n",
        "    lat_errors = np.abs(y_true[:, 0] - y_pred[:, 0])\n",
        "    lon_errors = np.abs(y_true[:, 1] - y_pred[:, 1])\n",
        "\n",
        "    last_indices = []\n",
        "    for uid in np.unique(users_test):\n",
        "        user_indices = np.where(users_test == uid)[0]\n",
        "        if user_indices.size > 0:\n",
        "            last_indices.append(user_indices[-1])\n",
        "    fde = np.mean(errors_meters[last_indices])\n",
        "\n",
        "    metrics = {\n",
        "        'ADE (m)': np.mean(errors_meters),\n",
        "        'FDE (m)': fde,\n",
        "        'Median Error (m)': np.median(errors_meters),\n",
        "        '95th Percentile (m)': np.percentile(errors_meters, 95),\n",
        "        'RMSE (m)': np.sqrt(np.mean(errors_meters**2)),\n",
        "        'Within-50m (%)': np.mean(errors_meters < 50) * 100,\n",
        "        'Within-100m (%)': np.mean(errors_meters < 100) * 100,\n",
        "        'Max Error (m)': np.max(errors_meters),\n",
        "        'Lat MAE (deg)': np.mean(lat_errors),\n",
        "        'Lon MAE (deg)': np.mean(lon_errors)\n",
        "    }\n",
        "\n",
        "    print(\"\\n=== Evaluation Metrics ===\")\n",
        "    for name, value in metrics.items():\n",
        "        if '%' in name:\n",
        "            print(f\"{name}: {value:.2f}%\")\n",
        "        elif 'deg' in name:\n",
        "            print(f\"{name}: {value:.6f}°\")\n",
        "        else:\n",
        "            print(f\"{name}: {value:.2f}\")\n",
        "\n",
        "    return metrics"
      ],
      "metadata": {
        "id": "q-EdYBM5-yX7"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict({'coord_input': X_test, 'user_input': u_test, 'time_input': t_test})\n",
        "evaluate_metrics(y_test, y_pred, u_test, scaler=scaler)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FxXN5Rso-zqv",
        "outputId": "517cd946-20de-4d9c-d7bc-75d26404f96e"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m17001/17001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 3ms/step\n",
            "\n",
            "=== Evaluation Metrics ===\n",
            "ADE (m): 611170.33\n",
            "FDE (m): 419605.44\n",
            "Median Error (m): 154373.79\n",
            "95th Percentile (m): 2066001.99\n",
            "RMSE (m): 1509503.13\n",
            "Within-50m (%): 0.00%\n",
            "Within-100m (%): 0.00%\n",
            "Max Error (m): 11579529.90\n",
            "Lat MAE (deg): 1.813007°\n",
            "Lon MAE (deg): 7.254320°\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'ADE (m)': np.float64(611170.329488708),\n",
              " 'FDE (m)': np.float64(419605.4438815654),\n",
              " 'Median Error (m)': np.float64(154373.79396036937),\n",
              " '95th Percentile (m)': np.float64(2066001.9912322746),\n",
              " 'RMSE (m)': np.float64(1509503.1261216982),\n",
              " 'Within-50m (%)': np.float64(0.0),\n",
              " 'Within-100m (%)': np.float64(0.000551459437401082),\n",
              " 'Max Error (m)': np.float64(11579529.899408773),\n",
              " 'Lat MAE (deg)': np.float64(1.813006687420709),\n",
              " 'Lon MAE (deg)': np.float64(7.254319758795673)}"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    }
  ]
}
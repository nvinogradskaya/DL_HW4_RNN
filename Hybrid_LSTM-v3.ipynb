{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nvinogradskaya/DL_HW4_RNN/blob/main/Hybrid_LSTM-v3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jdUPW6QySgOc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import uuid\n",
        "import shutil\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, Concatenate, Dropout, LayerNormalization\n",
        "from tensorflow.keras.utils import Sequence\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5wMNPu0yS5HF",
        "outputId": "9d7e1784-0a85-441b-bfb2-c5272cd9a827"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# ======== parameters ========\n",
        "MAX_USERS = 5\n",
        "SEQ_LENGTH = 10\n",
        "EMBEDDING_DIM = 16\n",
        "LSTM_UNITS = 64\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 10\n",
        "TEST_SIZE = 0.3\n",
        "\n",
        "DATA_PATH = \"/content/drive/My Drive/Colab Notebooks/Data/\"\n",
        "SAVE_PATH = \"/content/drive/My Drive/Colab Notebooks/contrastive_results-v3/\"\n",
        "SEQ_SAVE_PATH = os.path.join(SAVE_PATH, 'sequences/')\n",
        "os.makedirs(SAVE_PATH, exist_ok=True)\n",
        "os.makedirs(SEQ_SAVE_PATH, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2kNc_MUhTAd_"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "def load_and_preprocess_data(data_path, max_users=MAX_USERS):\n",
        "    data = []\n",
        "    user_dirs = sorted(os.listdir(data_path))[:max_users]\n",
        "    for user in tqdm(user_dirs, desc=\"Loading users\"):\n",
        "        traj_dir = os.path.join(data_path, user, 'Trajectory')\n",
        "        traj_files = sorted([f for f in os.listdir(traj_dir) if f.endswith('.plt')])\n",
        "        for traj_file in traj_files:\n",
        "            df = pd.read_csv(\n",
        "                os.path.join(traj_dir, traj_file),\n",
        "                skiprows=6,\n",
        "                header=None,\n",
        "                usecols=[0, 1, 3, 5, 6],\n",
        "                names=['lat', 'lon', 'alt', 'date', 'time']\n",
        "            )\n",
        "            df['user'] = user\n",
        "            data.append(df)\n",
        "\n",
        "    df = pd.concat(data, ignore_index=True)\n",
        "    df['datetime'] = pd.to_datetime(df['date'] + ' ' + df['time'])\n",
        "    df.sort_values(by=['user', 'datetime'], inplace=True)\n",
        "    df = df[(df['lat'] != 0) & (df['lon'] != 0)].ffill()\n",
        "\n",
        "    scaler = MinMaxScaler()\n",
        "    df[['lat', 'lon', 'alt']] = scaler.fit_transform(df[['lat', 'lon', 'alt']])\n",
        "\n",
        "    df['hour_sin'] = np.sin(2 * np.pi * df['datetime'].dt.hour / 24)\n",
        "    df['hour_cos'] = np.cos(2 * np.pi * df['datetime'].dt.hour / 24)\n",
        "    df['day_sin'] = np.sin(2 * np.pi * df['datetime'].dt.dayofweek / 7)\n",
        "    df['day_cos'] = np.cos(2 * np.pi * df['datetime'].dt.dayofweek / 7)\n",
        "\n",
        "    user_ids = {user: idx for idx, user in enumerate(df['user'].unique())}\n",
        "    df['user_id'] = df['user'].map(user_ids)\n",
        "\n",
        "    return df, user_ids, scaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HvR3nKZNTKB1",
        "outputId": "465cdda9-e395-42b5-c43d-f82d3a42f1b7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading users: 100%|██████████| 5/5 [00:18<00:00,  3.69s/it]\n"
          ]
        }
      ],
      "source": [
        "df, user_ids, scaler = load_and_preprocess_data(DATA_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FQ1doJOKTjHc"
      },
      "outputs": [],
      "source": [
        "# ======== Sequence Creation ========\n",
        "def create_sequences_and_save(df, user_ids, seq_length, test_size=0.3, save_path='./seq_data'):\n",
        "    os.makedirs(save_path, exist_ok=True)\n",
        "    features = ['lat', 'lon', 'alt', 'hour_sin', 'hour_cos', 'day_sin', 'day_cos']\n",
        "    targets = ['lat', 'lon']\n",
        "\n",
        "    for user, user_df in tqdm(df.groupby('user'), desc=\"Creating sequences\"):\n",
        "        uid = user_ids[user]\n",
        "        user_df = user_df.reset_index(drop=True)\n",
        "        split_idx = int(len(user_df) * (1 - test_size))\n",
        "        if split_idx <= seq_length:\n",
        "            continue\n",
        "\n",
        "        def process_chunk(data, is_train=True):\n",
        "            window_size = seq_length + 1\n",
        "            data_values = data[features].values\n",
        "            if len(data_values) < window_size:\n",
        "                return\n",
        "            X = np.lib.stride_tricks.sliding_window_view(data_values, (window_size, data_values.shape[1])).squeeze(axis=1)\n",
        "            X = X[:, :-1]\n",
        "            y = data[targets].values[seq_length:]\n",
        "            chunk_size = 1000\n",
        "            for i in range(0, len(X), chunk_size):\n",
        "                save_chunk(X[i:i+chunk_size], y[i:i+chunk_size], is_train)\n",
        "\n",
        "        def save_chunk(X, y, is_train):\n",
        "            suffix = 'train' if is_train else 'test'\n",
        "            chunk_id = uuid.uuid4().hex\n",
        "            np.savez_compressed(\n",
        "                os.path.join(save_path, f'user_{uid}_{suffix}_{chunk_id}.npz'),\n",
        "                X=X,\n",
        "                y=y,\n",
        "                user_id=uid\n",
        "            )\n",
        "\n",
        "        process_chunk(user_df.iloc[:split_idx], is_train=True)\n",
        "        process_chunk(user_df.iloc[split_idx-seq_length:], is_train=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CeARqJhKTmjj",
        "outputId": "2a6f5918-7a23-4379-8572-d1b86b2e1312"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Creating sequences: 100%|██████████| 5/5 [00:25<00:00,  5.03s/it]\n"
          ]
        }
      ],
      "source": [
        "shutil.rmtree(SEQ_SAVE_PATH, ignore_errors=True)\n",
        "os.makedirs(SEQ_SAVE_PATH, exist_ok=True)\n",
        "create_sequences_and_save(df, user_ids, SEQ_LENGTH, save_path=SEQ_SAVE_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xj5i4fCDTpRH"
      },
      "outputs": [],
      "source": [
        "# ======== Load Saved Sequences ========\n",
        "def load_all_sequences_from_disk(save_path):\n",
        "    X_train, X_test, y_train, y_test, users_train, users_test = [], [], [], [], [], []\n",
        "    for fname in tqdm(sorted(os.listdir(save_path)), desc=\"Loading sequences\"):\n",
        "        if not fname.endswith('.npz'):\n",
        "            continue\n",
        "        split_type = 'train' if 'train' in fname else 'test'\n",
        "        uid = int(fname.split('_')[1])\n",
        "        data = np.load(os.path.join(save_path, fname))\n",
        "        X, y = data['X'], data['y']\n",
        "        if split_type == 'train':\n",
        "            X_train.append(X); y_train.append(y); users_train.append(np.full(len(X), uid))\n",
        "        else:\n",
        "            X_test.append(X); y_test.append(y); users_test.append(np.full(len(X), uid))\n",
        "    return (\n",
        "        np.concatenate(X_train), np.concatenate(X_test),\n",
        "        np.concatenate(y_train), np.concatenate(y_test),\n",
        "        np.concatenate(users_train), np.concatenate(users_test)\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3DFmQFIQTtsn",
        "outputId": "aa335703-0e9d-45c6-d00d-da7f389fc3ac"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading sequences: 100%|██████████| 1460/1460 [00:16<00:00, 89.85it/s] \n"
          ]
        }
      ],
      "source": [
        "X_train, X_test, y_train, y_test, users_train, users_test = load_all_sequences_from_disk(SEQ_SAVE_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nO6QUfVoTwCG"
      },
      "outputs": [],
      "source": [
        "# ======== Contrastive Embedding Learning ========\n",
        "def create_triplets(X, user_ids):\n",
        "    anchors, positives, negatives = [], [], []\n",
        "    for uid in tqdm(np.unique(user_ids), desc=\"Creating triplets\"):\n",
        "        same_user_idx = np.where(user_ids == uid)[0]\n",
        "        diff_user_idx = np.where(user_ids != uid)[0]\n",
        "        if len(same_user_idx) < 2:\n",
        "            continue\n",
        "        for i in range(min(len(same_user_idx) - 1, 100)):\n",
        "            a_idx, p_idx = same_user_idx[i], same_user_idx[i+1]\n",
        "            n_idx = np.random.choice(diff_user_idx)\n",
        "            anchors.append(X[a_idx])\n",
        "            positives.append(X[p_idx])\n",
        "            negatives.append(X[n_idx])\n",
        "    return np.array(anchors), np.array(positives), np.array(negatives)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MBKFDynfTzDw",
        "outputId": "f4be126a-0056-4585-f3e3-a6f03ab61ca0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Creating triplets: 100%|██████████| 5/5 [00:00<00:00, 242.49it/s]\n"
          ]
        }
      ],
      "source": [
        "anchors, positives, negatives = create_triplets(X_train, users_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LoV2XKV7T1KZ"
      },
      "outputs": [],
      "source": [
        "def contrastive_model(input_shape, embedding_dim):\n",
        "    inp = Input(shape=input_shape)\n",
        "    x = LSTM(32)(inp)\n",
        "    x = Dense(embedding_dim)(x)\n",
        "    model = Model(inputs=inp, outputs=x)\n",
        "    return model\n",
        "\n",
        "def triplet_loss_fn(a, p, n, margin=1.0):\n",
        "    ap_dist = tf.reduce_sum(tf.square(a - p), axis=1)\n",
        "    an_dist = tf.reduce_sum(tf.square(a - n), axis=1)\n",
        "    return tf.reduce_mean(tf.maximum(ap_dist - an_dist + margin, 0.0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ufslJHfZT5X-"
      },
      "outputs": [],
      "source": [
        "triplet_encoder = contrastive_model(X_train.shape[1:], EMBEDDING_DIM)\n",
        "optimizer = tf.keras.optimizers.Adam(1e-3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eHHQwdcUT7Lk",
        "outputId": "da4f2a0c-04f4-4f7e-cc09-2cf42a53bac6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "сontrastive epoch 1 // loss = 0.1677\n",
            "сontrastive epoch 2 // loss = 0.1434\n",
            "сontrastive epoch 3 // loss = 0.1240\n",
            "сontrastive epoch 4 // loss = 0.1090\n",
            "сontrastive epoch 5 // loss = 0.0967\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(5):\n",
        "    with tf.GradientTape() as tape:\n",
        "        emb_a = triplet_encoder(anchors)\n",
        "        emb_p = triplet_encoder(positives)\n",
        "        emb_n = triplet_encoder(negatives)\n",
        "        loss = triplet_loss_fn(emb_a, emb_p, emb_n)\n",
        "    grads = tape.gradient(loss, triplet_encoder.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, triplet_encoder.trainable_variables))\n",
        "    print(f\"сontrastive epoch {epoch+1} // loss = {loss.numpy():.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rtyNa9O4UHj-"
      },
      "outputs": [],
      "source": [
        "user_embeddings_matrix = {}\n",
        "for uid in np.unique(users_train):\n",
        "    user_seqs = X_train[users_train == uid]\n",
        "    user_embs = triplet_encoder(user_seqs)\n",
        "    user_embeddings_matrix[uid] = tf.reduce_mean(user_embs, axis=0).numpy()\n",
        "\n",
        "user_embeddings_train = np.array([user_embeddings_matrix[uid] for uid in users_train])\n",
        "user_embeddings_test = np.array([user_embeddings_matrix[uid] for uid in users_test])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zt9FVC0CUQA8"
      },
      "outputs": [],
      "source": [
        "class CombinedDataGenerator(tf.keras.utils.Sequence):\n",
        "    def __init__(self, X, user_embs, y, batch_size=64):\n",
        "        self.X = X\n",
        "        self.user_embs = user_embs\n",
        "        self.y = y\n",
        "        self.batch_size = batch_size\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(len(self.X) / self.batch_size))\n",
        "    def __getitem__(self, idx):\n",
        "        start = idx * self.batch_size\n",
        "        end = min((idx + 1) * self.batch_size, len(self.X))\n",
        "        X_batch = self.X[start:end]\n",
        "        user_emb_batch = self.user_embs[start:end]\n",
        "        y_batch = self.y[start:end]\n",
        "        user_emb_expanded = np.repeat(user_emb_batch[:, np.newaxis, :], X_batch.shape[1], axis=1)\n",
        "        combined_X = np.concatenate([X_batch, user_emb_expanded], axis=-1)\n",
        "        return combined_X.astype(np.float32), y_batch.astype(np.float32)\n",
        "\n",
        "def build_lstm_model(input_shape):\n",
        "    seq_input = Input(shape=input_shape)\n",
        "    x = LSTM(LSTM_UNITS, return_sequences=False)(seq_input)\n",
        "    x = LayerNormalization()(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "    x = Dense(64, activation='relu')(x)\n",
        "    x = Dense(32, activation='relu')(x)\n",
        "    output = Dense(2, activation='linear')(x)\n",
        "    model = Model(inputs=seq_input, outputs=output)\n",
        "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VXb9qFmyURCm"
      },
      "outputs": [],
      "source": [
        "train_gen = CombinedDataGenerator(X_train, user_embeddings_train, y_train, batch_size=BATCH_SIZE)\n",
        "val_gen = CombinedDataGenerator(X_test, user_embeddings_test, y_test, batch_size=BATCH_SIZE)\n",
        "input_shape = (X_train.shape[1], X_train.shape[2] + EMBEDDING_DIM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xHGghDajUYUg"
      },
      "outputs": [],
      "source": [
        "model = build_lstm_model(input_shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4lKXTlVyUaLt",
        "outputId": "03204c75-8cde-4ce1-9029-2a5137d71216"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m15917/15917\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m161s\u001b[0m 10ms/step - loss: 0.0025 - mae: 0.0288 - val_loss: 0.0034 - val_mae: 0.0492\n",
            "Epoch 2/5\n",
            "\u001b[1m15917/15917\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m169s\u001b[0m 11ms/step - loss: 6.2599e-04 - mae: 0.0124 - val_loss: 0.0027 - val_mae: 0.0467\n",
            "Epoch 3/5\n",
            "\u001b[1m15917/15917\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m155s\u001b[0m 10ms/step - loss: 3.4207e-04 - mae: 0.0096 - val_loss: 0.0020 - val_mae: 0.0376\n",
            "Epoch 4/5\n",
            "\u001b[1m15917/15917\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m163s\u001b[0m 10ms/step - loss: 2.5655e-04 - mae: 0.0081 - val_loss: 0.0021 - val_mae: 0.0381\n",
            "Epoch 5/5\n",
            "\u001b[1m15917/15917\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m195s\u001b[0m 10ms/step - loss: 2.1453e-04 - mae: 0.0075 - val_loss: 0.0023 - val_mae: 0.0403\n"
          ]
        }
      ],
      "source": [
        "callbacks = [\n",
        "    ModelCheckpoint(os.path.join(SAVE_PATH, 'best_model.keras'), save_best_only=True, monitor='val_loss'),\n",
        "    EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "]\n",
        "history = model.fit(train_gen, validation_data=val_gen, epochs=5, callbacks=callbacks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JX3V4f71UdJE"
      },
      "outputs": [],
      "source": [
        "model.load_weights(os.path.join(SAVE_PATH, 'best_model.keras'))\n",
        "def predict_with_generator(model, X, user_embs, batch_size=64, save_path=None):\n",
        "    generator = CombinedDataGenerator(X, user_embs, np.zeros((len(user_embs), 2)), batch_size=batch_size)\n",
        "    preds = []\n",
        "\n",
        "    for i, (X_batch, _) in enumerate(tqdm(generator, total=len(generator), desc=\"Predicting\")):\n",
        "        try:\n",
        "            # Проверка на пустой батч\n",
        "            if X_batch[0].shape[0] == 0:\n",
        "                print(f\"⚠️ Пустой батч {i}, пропускаем.\")\n",
        "                continue\n",
        "\n",
        "            batch_pred = model.predict(X_batch, verbose=0)\n",
        "            preds.append(batch_pred)\n",
        "\n",
        "            # Периодическое сохранение\n",
        "            if save_path and (i + 1) % 1000 == 0:\n",
        "                partial_preds = np.vstack(preds)\n",
        "                np.save(f\"{save_path}_partial_{i+1}.npy\", partial_preds)\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Ошибка на батче {i}: {e}\")\n",
        "            continue\n",
        "\n",
        "    if preds:\n",
        "        final_preds = np.vstack(preds)\n",
        "        if save_path:\n",
        "            np.save(f\"{save_path}_final.npy\", final_preds)\n",
        "            print(f\"✅ Итоговое предсказание сохранено в: {save_path}_final.npy\")\n",
        "        return final_preds\n",
        "    else:\n",
        "        print(\"⚠️ Нет предсказаний — возвращаю пустой массив.\")\n",
        "        return np.empty((0, 2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "o0sQn5bgUmFK",
        "outputId": "e52aefb9-9428-4617-8b8a-c6a51e24b95e"
      },
      "outputs": [],
      "source": [
        "y_pred = predict_with_generator(model, X_test, user_embeddings_test, save_path=os.path.join(SAVE_PATH, \"y_pred\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "id": "M1efNn40UoUu",
        "outputId": "226ba740-35b3-4d1e-bcfc-810b79b4c2a3"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'y_pred' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-4efda1bb3092>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0made\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfde\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwithin_100m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0made\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfde\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc_within_100m\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"ADE: {ade:.4f}, FDE: {fde:.4f}, % predictions < 100m: {acc_within_100m:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'y_pred' is not defined"
          ]
        }
      ],
      "source": [
        "def calculate_metrics(preds, targets):\n",
        "    ade = np.mean(np.linalg.norm(preds - targets, axis=-1))\n",
        "    fde = np.mean(np.linalg.norm(preds - targets, axis=-1))  # Final error is the same in single-step prediction\n",
        "    within_100m = np.mean(np.linalg.norm(preds - targets, axis=-1) < 0.001)  # 0.001 ≈ 100 meters\n",
        "    return ade, fde, within_100m\n",
        "\n",
        "ade, fde, acc_within_100m = calculate_metrics(y_pred, y_test)\n",
        "print(f\"ADE: {ade:.4f}, FDE: {fde:.4f}, % predictions < 100m: {acc_within_100m:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JZ9RkzDFUsDL"
      },
      "outputs": [],
      "source": [
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.legend()\n",
        "plt.title('Training Losses')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOfuN9YhPi3hK9FcBLF0gwi",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
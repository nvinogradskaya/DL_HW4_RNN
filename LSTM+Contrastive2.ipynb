{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPR2Ybmc6ID5l88F2sPWWBi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nvinogradskaya/DL_HW4_RNN/blob/main/LSTM%2BContrastive2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c2M8C_JsKWBR"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import LSTM, Dense, Embedding, Concatenate, Input, Reshape\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "SEQ_LENGTH = 10\n",
        "EMBEDDING_DIM = 16\n",
        "LSTM_UNITS = 64\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 5\n",
        "TEST_SIZE = 0.2\n",
        "drive.mount('/content/drive')\n",
        "DATA_PATH = \"/content/drive/My Drive/Colab Notebooks/Data/\"\n",
        "SAVE_PATH = \"/content/drive/My Drive/Colab Notebooks/contrastive_results/\"\n",
        "SEQ_SAVE_PATH = os.path.join(SAVE_PATH, 'sequences/')\n",
        "os.makedirs(SAVE_PATH, exist_ok=True)\n",
        "os.makedirs(SEQ_SAVE_PATH, exist_ok=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Aei32AOGKCn",
        "outputId": "2705c77a-01a9-43d9-cb27-eccb9c4aca21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_preprocess_data(data_path, max_users=5, max_files_per_user=10):  # <- Добавлено\n",
        "    data = []\n",
        "    user_dirs = sorted(os.listdir(data_path))[:max_users]\n",
        "\n",
        "    for user in user_dirs:\n",
        "        traj_dir = os.path.join(data_path, user, 'Trajectory')\n",
        "        traj_files = sorted([f for f in os.listdir(traj_dir) if f.endswith('.plt')])[:max_files_per_user]  # <- Ограничение\n",
        "\n",
        "        for traj_file in traj_files:\n",
        "            df = pd.read_csv(\n",
        "                os.path.join(traj_dir, traj_file),\n",
        "                skiprows=6,\n",
        "                header=None,\n",
        "                usecols=[0, 1, 3, 5, 6],\n",
        "                names=['lat', 'lon', 'alt', 'date', 'time']\n",
        "            )\n",
        "            df['user'] = user\n",
        "            data.append(df)\n",
        "\n",
        "    df = pd.concat(data, ignore_index=True)\n",
        "    df['datetime'] = pd.to_datetime(df['date'] + ' ' + df['time'])\n",
        "    df.sort_values(by=['user', 'datetime'], inplace=True)\n",
        "    df = df[(df['lat'] != 0) & (df['lon'] != 0)].ffill()\n",
        "\n",
        "    scaler = MinMaxScaler()\n",
        "    df[['lat', 'lon', 'alt']] = scaler.fit_transform(df[['lat', 'lon', 'alt']])\n",
        "\n",
        "    df['hour_sin'] = np.sin(2 * np.pi * df['datetime'].dt.hour / 24)\n",
        "    df['hour_cos'] = np.cos(2 * np.pi * df['datetime'].dt.hour / 24)\n",
        "    df['day_sin'] = np.sin(2 * np.pi * df['datetime'].dt.dayofweek / 7)\n",
        "    df['day_cos'] = np.cos(2 * np.pi * df['datetime'].dt.dayofweek / 7)\n",
        "\n",
        "    user_ids = {user: idx for idx, user in enumerate(df['user'].unique())}\n",
        "    df['user_id'] = df['user'].map(user_ids)\n",
        "\n",
        "    return df, user_ids, scaler"
      ],
      "metadata": {
        "id": "7Cq_kIMrG4i_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df, user_ids, scaler = load_and_preprocess_data(DATA_PATH, max_users=10, max_files_per_user=10)"
      ],
      "metadata": {
        "id": "_Y3G-kOMG_pE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_sequences_and_save(df, user_ids, seq_length, test_size=0.2, save_path='./seq_data'):\n",
        "    os.makedirs(save_path, exist_ok=True)\n",
        "    features = ['lat', 'lon', 'alt', 'hour_sin', 'hour_cos', 'day_sin', 'day_cos']\n",
        "    targets = ['lat', 'lon']\n",
        "\n",
        "    # Обработка пользователей по одному\n",
        "    for user, user_df in df.groupby('user'):\n",
        "        uid = user_ids[user]\n",
        "        user_df = user_df.reset_index(drop=True)\n",
        "        split_idx = int(len(user_df) * (1 - test_size))\n",
        "        if split_idx <= seq_length:\n",
        "            continue\n",
        "\n",
        "        # Генерация данных чанками\n",
        "        def process_chunk(data, is_train=True):\n",
        "        # Используем скользящее окно для создания последовательностей\n",
        "        window_size = seq_length + 1\n",
        "        data_values = data[features].values\n",
        "        data_targets = data[targets].values\n",
        "\n",
        "        # Создаем окна\n",
        "        X = np.lib.stride_tricks.sliding_window_view(data_values, window_shape=(window_size, data_values.shape[1]))\n",
        "        X = X[:, :-1]  # Берем первые SEQ_LENGTH шагов\n",
        "        y = data_targets[seq_length:]  # Целевые значения\n",
        "\n",
        "        # Сохраняем чанки\n",
        "        chunk_size = 1000  # Увеличьте размер чанка, если нужно\n",
        "        for i in range(0, len(X), chunk_size):\n",
        "            save_chunk(X[i:i+chunk_size], y[i:i+chunk_size], is_train)\n",
        "\n",
        "        return [], []  # Данные уже сохранены\n",
        "\n",
        "        # Функция сохранения чанков\n",
        "        def save_chunk(X, y, is_train):\n",
        "            suffix = 'train' if is_train else 'test'\n",
        "            chunk_id = uuid.uuid4().hex\n",
        "            np.savez_compressed(\n",
        "                os.path.join(save_path, f'user_{uid}_{suffix}_{chunk_id}.npz'),\n",
        "                X=np.array(X),\n",
        "                y=np.array(y)\n",
        "            )\n",
        "\n",
        "        # Обработка train и test\n",
        "        process_chunk(user_df.iloc[:split_idx], is_train=True)\n",
        "        process_chunk(user_df.iloc[split_idx-seq_length:], is_train=False)"
      ],
      "metadata": {
        "id": "7AQ_kfm61O80"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import uuid\n",
        "create_sequences_and_save(df, user_ids, SEQ_LENGTH, save_path=SEQ_SAVE_PATH)"
      ],
      "metadata": {
        "id": "tE535dYW1Pq_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_all_sequences_from_disk(save_path):\n",
        "    X_train, X_test, y_train, y_test, users_train, users_test = [], [], [], [], [], []\n",
        "    for fname in sorted(os.listdir(save_path)):\n",
        "        if not fname.endswith('.npz'):\n",
        "            continue\n",
        "        data = np.load(os.path.join(save_path, fname))\n",
        "        X_train.append(data['X_train'])\n",
        "        y_train.append(data['y_train'])\n",
        "        users_train.append(np.full(len(data['X_train']), data['user_id']))\n",
        "        X_test.append(data['X_test'])\n",
        "        y_test.append(data['y_test'])\n",
        "        users_test.append(np.full(len(data['X_test']), data['user_id']))\n",
        "\n",
        "    return (\n",
        "        np.concatenate(X_train), np.concatenate(X_test),\n",
        "        np.concatenate(y_train), np.concatenate(y_test),\n",
        "        np.concatenate(users_train), np.concatenate(users_test)\n",
        "    )"
      ],
      "metadata": {
        "id": "22S4yFMPpoYb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test, users_train, users_test = load_all_sequences_from_disk(SEQ_SAVE_PATH)"
      ],
      "metadata": {
        "id": "dMqM1fpmpqQZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ContrastiveModel(tf.keras.Model):\n",
        "    def __init__(self, num_users, embedding_dim):\n",
        "        super().__init__()\n",
        "        self.embedding = Embedding(num_users, embedding_dim)\n",
        "        self.dense = Dense(embedding_dim, activation='tanh')\n",
        "\n",
        "    def call(self, inputs):\n",
        "        user_id = inputs\n",
        "        user_emb = self.embedding(user_id)\n",
        "        return self.dense(user_emb)\n",
        "\n",
        "    def train_step(self, data):\n",
        "        users, _ = data\n",
        "        with tf.GradientTape() as tape:\n",
        "            embeddings = self(users)\n",
        "            anchor = embeddings[:, 0]\n",
        "            positive = embeddings[:, 1]\n",
        "            distances = tf.reduce_sum(tf.square(anchor - positive), axis=-1)\n",
        "            loss = self.compiled_loss(None, distances)\n",
        "\n",
        "        gradients = tape.gradient(loss, self.trainable_variables)\n",
        "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
        "        return {'loss': loss}"
      ],
      "metadata": {
        "id": "-tyAEHfmLQZ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "contrastive_model = ContrastiveModel(num_users=len(user_ids), embedding_dim=EMBEDDING_DIM)\n",
        "contrastive_model.compile(optimizer=Adam(0.001), loss=tf.keras.losses.MeanSquaredError())\n",
        "contrastive_model.fit(users_train, np.zeros(len(users_train)), epochs=5, batch_size=BATCH_SIZE)\n",
        "user_embeddings = contrastive_model.predict(np.unique(users_train))"
      ],
      "metadata": {
        "id": "_BH_OcNcLYzZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def combine_features(X, users, embeddings):\n",
        "    embeddings_expanded = np.repeat(embeddings[users][:, np.newaxis, :], SEQ_LENGTH, axis=1)\n",
        "    return np.concatenate([X, embeddings_expanded], axis=-1)\n",
        "\n",
        "X_train_combined = combine_features(X_train, users_train, user_embeddings)\n",
        "X_test_combined = combine_features(X_test, users_test, user_embeddings)"
      ],
      "metadata": {
        "id": "8q4GE2u8LenV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_lstm_model(seq_length, embedding_dim, num_features, num_users):\n",
        "    seq_input = Input(shape=(seq_length, num_features))\n",
        "    user_input = Input(shape=(1,), dtype=tf.int32)\n",
        "\n",
        "    user_emb = Embedding(num_users, embedding_dim)(user_input)\n",
        "    user_emb = Reshape((embedding_dim,))(user_emb)\n",
        "\n",
        "    lstm_out = LSTM(LSTM_UNITS, return_sequences=False)(seq_input)\n",
        "    combined = Concatenate()([lstm_out, user_emb])\n",
        "\n",
        "    dense_out = Dense(64, activation='relu')(combined)\n",
        "    dense_out = Dense(32, activation='relu')(dense_out)\n",
        "    final_output = Dense(2, activation='linear')(dense_out)\n",
        "\n",
        "    model = Model(inputs=[seq_input, user_input], outputs=final_output)\n",
        "    model.compile(optimizer=Adam(0.001), loss='mse', metrics=['mae'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "pkBKMmTWLji6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sequence_generator(file_list, batch_size, seq_length, embedding_matrix):\n",
        "    def generator():\n",
        "        for file in file_list:\n",
        "            data = np.load(file)\n",
        "            X = data['X_train']\n",
        "            y = data['y_train']\n",
        "            user_id = data['user_id']\n",
        "            emb = embedding_matrix[user_id]\n",
        "            emb_expanded = np.repeat(emb[np.newaxis, :], seq_length, axis=0)\n",
        "\n",
        "            for i in range(0, len(X), batch_size):\n",
        "                X_batch = X[i:i+batch_size]\n",
        "                y_batch = y[i:i+batch_size]\n",
        "                u_batch = np.array([user_id] * len(X_batch))\n",
        "                emb_batch = np.repeat(emb[np.newaxis, :], len(X_batch) * seq_length, axis=0).reshape(len(X_batch), seq_length, -1)\n",
        "                yield (np.concatenate([X_batch, emb_batch], axis=-1), u_batch), y_batch\n",
        "    return generator"
      ],
      "metadata": {
        "id": "p2S6sExuqVs3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataset_from_generator(npz_folder, batch_size, seq_length, embedding_matrix, split='train'):\n",
        "    file_list = sorted([os.path.join(npz_folder, f) for f in os.listdir(npz_folder) if f.endswith('.npz')])\n",
        "\n",
        "    def dataset_gen():\n",
        "        for file in file_list:\n",
        "            data = np.load(file)\n",
        "            X = data[f'X_{split}']\n",
        "            y = data[f'y_{split}']\n",
        "            user_id = data['user_id']\n",
        "            emb = embedding_matrix[user_id]\n",
        "            emb_expanded = np.repeat(emb[np.newaxis, :], seq_length, axis=0)\n",
        "\n",
        "            for i in range(0, len(X), batch_size):\n",
        "                X_batch = X[i:i+batch_size]\n",
        "                y_batch = y[i:i+batch_size]\n",
        "                u_batch = np.array([user_id] * len(X_batch))\n",
        "                emb_batch = np.repeat(emb[np.newaxis, :], len(X_batch) * seq_length, axis=0).reshape(len(X_batch), seq_length, -1)\n",
        "                yield (np.concatenate([X_batch, emb_batch], axis=-1), u_batch), y_batch\n",
        "\n",
        "    output_signature = (\n",
        "        (\n",
        "            tf.TensorSpec(shape=(None, seq_length, X.shape[2] + embedding_matrix.shape[1]), dtype=tf.float32),\n",
        "            tf.TensorSpec(shape=(None,), dtype=tf.int32)\n",
        "        ),\n",
        "        tf.TensorSpec(shape=(None, 2), dtype=tf.float32)\n",
        "    )\n",
        "\n",
        "    return tf.data.Dataset.from_generator(\n",
        "        dataset_gen,\n",
        "        output_signature=output_signature\n",
        "    ).prefetch(tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "YRFK9ommqW4-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = create_dataset_from_generator(SEQ_SAVE_PATH, BATCH_SIZE, SEQ_LENGTH, user_embeddings, split='train')\n",
        "test_dataset = create_dataset_from_generator(SEQ_SAVE_PATH, BATCH_SIZE, SEQ_LENGTH, user_embeddings, split='test')\n",
        "\n",
        "model = build_lstm_model(SEQ_LENGTH, EMBEDDING_DIM, X_train.shape[2] + EMBEDDING_DIM, len(user_ids))\n",
        "\n",
        "checkpoint = ModelCheckpoint(os.path.join(SAVE_PATH, 'best_model.h5'),\n",
        "                             save_best_only=True, monitor='val_loss')\n",
        "\n",
        "history = model.fit(train_dataset,\n",
        "          validation_data=test_dataset,\n",
        "          epochs=EPOCHS,\n",
        "          callbacks=[checkpoint])"
      ],
      "metadata": {
        "id": "U_DeKqGLqaW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_lstm_model(SEQ_LENGTH, EMBEDDING_DIM, X_train_combined.shape[2], len(user_ids))\n",
        "\n",
        "checkpoint = ModelCheckpoint(os.path.join(SAVE_PATH, 'best_model.h5'),\n",
        "                             save_best_only=True, monitor='val_loss')\n",
        "\n",
        "history = model.fit(\n",
        "    [X_train_combined, users_train], y_train,\n",
        "    validation_split=0.2,\n",
        "    epochs=EPOCHS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    callbacks=[checkpoint]\n",
        ")"
      ],
      "metadata": {
        "id": "aLTm4jCjLpp-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_metrics(preds, targets):\n",
        "    ade = np.mean(np.linalg.norm(preds - targets, axis=-1))\n",
        "    fde = np.linalg.norm(preds[:, -1] - targets[:, -1], axis=-1).mean()\n",
        "    distances = np.linalg.norm(preds[:, None] - targets[:, :, None], axis=-1)\n",
        "    acc1 = np.mean(np.argmin(distances, axis=-1) == 0)\n",
        "    return ade, fde, acc1\n",
        "\n",
        "model.load_weights(os.path.join(SAVE_PATH, 'best_model.h5'))\n",
        "y_pred = model.predict([X_test_combined, users_test])\n",
        "\n",
        "ade, fde, acc1 = calculate_metrics(y_pred, y_test)\n",
        "print(f\"ADE: {ade:.4f}, FDE: {fde:.4f}, Accuracy@1: {acc1:.4f}\")\n",
        "\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "P2ChXk7DLv0K"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
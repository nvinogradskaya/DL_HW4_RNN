{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNZ/EixwR90MqKjyIxLJcjA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nvinogradskaya/DL_HW4_RNN/blob/main/Untitled2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pCVDzt9K_PRV"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tqdm.auto import tqdm\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding, Concatenate, LayerNormalization, Attention\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from geopy.distance import geodesic\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_USERS = 170\n",
        "SEQ_LENGTH = 10\n",
        "LSTM_UNITS = 128\n",
        "BATCH_SIZE = 128\n",
        "EPOCHS = 20\n",
        "TEST_SIZE = 0.3\n",
        "EMBEDDING_DIM = 32"
      ],
      "metadata": {
        "id": "eaAKLqFG_l5b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data_with_stay_points(data_path):\n",
        "    data = []\n",
        "    user_map = {}\n",
        "    user_dirs = sorted(os.listdir(data_path))[:MAX_USERS]\n",
        "\n",
        "    for idx, user in enumerate(tqdm(user_dirs, desc=\"Users\")):\n",
        "        user_map[user] = idx\n",
        "        traj_dir = os.path.join(data_path, user, 'Trajectory')\n",
        "\n",
        "        # Параллельная обработка файлов\n",
        "        plt_files = [f for f in os.listdir(traj_dir) if f.endswith('.plt')]\n",
        "        for file in plt_files:\n",
        "            df = pd.read_csv(\n",
        "                os.path.join(traj_dir, file),\n",
        "                skiprows=6,\n",
        "                header=None,\n",
        "                usecols=[0, 1, 3, 5, 6],\n",
        "                names=['lat', 'lon', 'alt', 'date', 'time']\n",
        "            )\n",
        "            # Ускорение обработки datetime\n",
        "            df['datetime'] = pd.to_datetime(df['date'] + ' ' + df['time'], errors='coerce')\n",
        "            df.dropna(subset=['datetime'], inplace=True)\n",
        "            df['user_id'] = idx\n",
        "\n",
        "            # Обновленные параметры stay points\n",
        "            stay_df = detect_stay_points(df, dist_thresh=50, time_thresh=600)\n",
        "            if stay_df is not None and len(stay_df) > SEQ_LENGTH:\n",
        "                data.append(stay_df)\n",
        "\n",
        "    if not data:\n",
        "        raise ValueError(\"No valid data after processing\")\n",
        "\n",
        "    df_all = pd.concat(data, ignore_index=True)\n",
        "\n",
        "    # Исправленная фильтрация\n",
        "    df_all = df_all[\n",
        "        (df_all.lat.between(-90, 90)) &\n",
        "        (df_all.lon.between(-180, 180)) &\n",
        "        ~((df_all.lat == 0) & (df_all.lon == 0))\n",
        "    ].dropna()\n",
        "\n",
        "    # Корректная нормализация\n",
        "    scaler = MinMaxScaler()\n",
        "    df_all[['lat', 'lon', 'alt']] = scaler.fit_transform(df_all[['lat', 'lon', 'alt']])\n",
        "\n",
        "    # Отладочная информация\n",
        "    print(f\"\\nLoaded {len(df_all)} points from {len(user_map)} users\")\n",
        "    print(f\"Average points per user: {len(df_all)/len(user_map):.1f}\")\n",
        "\n",
        "    return df_all, user_map, scaler"
      ],
      "metadata": {
        "id": "mfeLkuA4_rsb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_stay_points(df, dist_thresh=50, time_thresh=600):\n",
        "    stays = []\n",
        "    i = 0\n",
        "    while i < len(df)-1:\n",
        "        j = i+1\n",
        "        valid_points = 0\n",
        "\n",
        "        while j < len(df):\n",
        "            try:\n",
        "                d = geodesic((df.iloc[i].lat, df.iloc[i].lon),\n",
        "                            (df.iloc[j].lat, df.iloc[j].lon)).meters\n",
        "                t = (df.iloc[j].datetime - df.iloc[i].datetime).total_seconds()\n",
        "\n",
        "                if d > dist_thresh or t > time_thresh:\n",
        "                    if valid_points >= 3:  # Минимум 3 точки для кластера\n",
        "                        stays.append({\n",
        "                            'lat': df.iloc[i:j].lat.mean(),\n",
        "                            'lon': df.iloc[i:j].lon.mean(),\n",
        "                            'alt': df.iloc[i:j].alt.mean(),\n",
        "                            'datetime': df.iloc[i].datetime,\n",
        "                            'user_id': df.iloc[i].user_id\n",
        "                        })\n",
        "                    break\n",
        "                valid_points += 1\n",
        "                j += 1\n",
        "            except:\n",
        "                j += 1\n",
        "        i = j if j > i else i+1\n",
        "\n",
        "    return pd.DataFrame(stays) if stays else None"
      ],
      "metadata": {
        "id": "SyTg6JaI_ur9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Обновленная функция create_sequences\n",
        "def create_sequences(df, seq_length=SEQ_LENGTH, step=1, test_size=TEST_SIZE):\n",
        "    X_train, X_test = [], []\n",
        "    y_train, y_test = [], []\n",
        "    u_train, u_test = [], []\n",
        "    t_train, t_test = [], []\n",
        "\n",
        "    df = df.sort_values(['user_id', 'datetime'])\n",
        "    for uid in tqdm(df.user_id.unique(), desc=\"Creating sequences\"):\n",
        "        user_df = df[df.user_id == uid]\n",
        "        if len(user_df) < seq_length + 1:\n",
        "            continue\n",
        "\n",
        "        # Разделение траекторий пользователя\n",
        "        split_idx = int(len(user_df) * (1 - test_size))\n",
        "        train_user = user_df.iloc[:split_idx]\n",
        "        test_user = user_df.iloc[split_idx:]\n",
        "\n",
        "        # Генерация последовательностей для train\n",
        "        for i in range(0, len(train_user) - seq_length, step):\n",
        "            seq = train_user.iloc[i:i+seq_length]\n",
        "            target = train_user.iloc[i+seq_length]\n",
        "            X_train.append(seq[['lat', 'lon', 'alt']].values)\n",
        "            y_train.append(target[['lat', 'lon']].values)\n",
        "            u_train.append(uid)\n",
        "            t_train.append(target['datetime'].hour)\n",
        "\n",
        "        # Генерация последовательностей для test\n",
        "        for i in range(0, len(test_user) - seq_length, step):\n",
        "            seq = test_user.iloc[i:i+seq_length]\n",
        "            target = test_user.iloc[i+seq_length]\n",
        "            X_test.append(seq[['lat', 'lon', 'alt']].values)\n",
        "            y_test.append(target[['lat', 'lon']].values)\n",
        "            u_test.append(uid)\n",
        "            t_test.append(target['datetime'].hour)\n",
        "\n",
        "    # Конвертация в numpy arrays\n",
        "    X_train = np.array(X_train)\n",
        "    X_test = np.array(X_test)\n",
        "    y_train = np.array(y_train)\n",
        "    y_test = np.array(y_test)\n",
        "    u_train = np.array(u_train)\n",
        "    u_test = np.array(u_test)\n",
        "    t_train = np.array(t_train)\n",
        "    t_test = np.array(t_test)\n",
        "\n",
        "    print(f\"\\nTrain sequences: {len(X_train)} | Test sequences: {len(X_test)}\")\n",
        "    return X_train, X_test, y_train, y_test, u_train, u_test, t_train, t_test"
      ],
      "metadata": {
        "id": "ri-rwLpl_xkl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Lambda\n",
        "\n",
        "from tensorflow.keras.layers import Bidirectional, MultiHeadAttention\n",
        "\n",
        "# Новые параметры\n",
        "LSTM_UNITS = 256  # Увеличили размерность\n",
        "ATTN_HEADS = 4    # Количество голов внимания\n",
        "KEY_DIM = 64      # Размерность ключей/запросов\n",
        "\n",
        "from tensorflow.keras.layers import Lambda\n",
        "from tensorflow.keras.layers import Lambda, GlobalAveragePooling1D\n",
        "\n",
        "def build_deepmove_model():\n",
        "    # Входные слои\n",
        "    coord_input = Input(shape=(SEQ_LENGTH, 3), name='coord_input')\n",
        "    user_input = Input(shape=(), dtype='int32', name='user_input')\n",
        "    time_input = Input(shape=(), dtype='int32', name='time_input')\n",
        "\n",
        "    # Эмбеддинги контекста\n",
        "    user_emb = Embedding(MAX_USERS, EMBEDDING_DIM)(user_input)\n",
        "    time_emb = Embedding(24, EMBEDDING_DIM)(time_input)\n",
        "\n",
        "    # 1. Двунаправленный LSTM\n",
        "    lstm_out = Bidirectional(\n",
        "        LSTM(LSTM_UNITS, return_sequences=True),\n",
        "        merge_mode='concat'\n",
        "    )(coord_input)\n",
        "\n",
        "    # 2. Иерархическое внимание\n",
        "    spatial_attention = MultiHeadAttention(\n",
        "        num_heads=ATTN_HEADS,\n",
        "        key_dim=KEY_DIM\n",
        "    )(lstm_out, lstm_out)\n",
        "\n",
        "    temporal_attention = MultiHeadAttention(\n",
        "        num_heads=ATTN_HEADS,\n",
        "        key_dim=KEY_DIM\n",
        "    )(spatial_attention, spatial_attention)\n",
        "\n",
        "    # 3. Объединение с контекстом\n",
        "    context_concat = Concatenate()([user_emb, time_emb])\n",
        "    context = Lambda(lambda x: tf.expand_dims(x, axis=1))(context_concat)  # Добавление оси\n",
        "\n",
        "    context_attention = MultiHeadAttention(\n",
        "        num_heads=ATTN_HEADS//2,\n",
        "        key_dim=KEY_DIM\n",
        "    )(temporal_attention, context)\n",
        "\n",
        "    # 4. Аггрегация и выход (исправлено)\n",
        "    attn_flat = Lambda(lambda x: tf.reduce_mean(x, axis=1))(context_attention)  # Обертка для reduce_mean\n",
        "    output = Dense(2, activation='linear')(attn_flat)\n",
        "\n",
        "    model = Model(\n",
        "        inputs=[coord_input, user_input, time_input],\n",
        "        outputs=output\n",
        "    )\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(0.001),\n",
        "        loss='mse',\n",
        "        metrics=['mae']\n",
        "    )\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "8jOor1n7_1rf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_metrics(y_true, y_pred):\n",
        "    def latlon_to_meters(lat1, lon1, lat2, lon2):\n",
        "        return np.array([geodesic((a, b), (c, d)).meters for a, b, c, d in zip(lat1, lon1, lat2, lon2)])\n",
        "\n",
        "    lat1, lon1 = y_true[:, 0], y_true[:, 1]\n",
        "    lat2, lon2 = y_pred[:, 0], y_pred[:, 1]\n",
        "    errors = latlon_to_meters(lat1, lon1, lat2, lon2)\n",
        "    ade = np.mean(errors)\n",
        "    fde = errors[-1]\n",
        "    pct_100m = np.mean(errors < 100) * 100\n",
        "    print(f\"\\nADE: {ade:.4f} m | FDE: {fde:.4f} m | % < 100m: {pct_100m:.8f}%\")"
      ],
      "metadata": {
        "id": "W-UxtA1r_4xE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "data_path = \"/content/drive/My Drive/Colab Notebooks/Data/\""
      ],
      "metadata": {
        "id": "SpxaHaY1_7t7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df, user_map, scaler = load_data_with_stay_points(data_path)"
      ],
      "metadata": {
        "id": "mtIoQdmrAG7v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test, u_train, u_test, t_train, t_test = create_sequences(df)"
      ],
      "metadata": {
        "id": "dxp7BNHrAQ71"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_deepmove_model()\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "KYSI6F_a3o2E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = X_train.astype(np.float32)\n",
        "y_train = y_train.astype(np.float32)\n",
        "u_train = u_train.astype(np.int32)\n",
        "t_train = t_train.astype(np.int32)\n",
        "\n",
        "X_test = X_test.astype(np.float32)\n",
        "y_test = y_test.astype(np.float32)\n",
        "u_test = u_test.astype(np.int32)\n",
        "t_test = t_test.astype(np.int32)\n"
      ],
      "metadata": {
        "id": "mN0bU1DoCiTI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "SAVE_PATH = \"/content/drive/My Drive/deepmove_results-v4/\"\n",
        "os.makedirs(SAVE_PATH, exist_ok=True)\n",
        "\n",
        "checkpoint_path = os.path.join(SAVE_PATH, \"best_model.h5\")\n",
        "\n",
        "callbacks = [\n",
        "    EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True),\n",
        "    ModelCheckpoint(filepath=checkpoint_path, monitor='val_loss', save_best_only=True, verbose=1)\n",
        "]"
      ],
      "metadata": {
        "id": "tv5-QgqEFWfU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    {'coord_input': X_train, 'user_input': u_train, 'time_input': t_train},\n",
        "    y_train,\n",
        "    validation_split=0.1,\n",
        "    epochs=EPOCHS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    callbacks=callbacks\n",
        ")"
      ],
      "metadata": {
        "id": "34cFJo5Q1bT1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_weights(checkpoint_path)"
      ],
      "metadata": {
        "id": "Ee13CSEaFlVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_training_history(history):\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['loss'], label='Train Loss')\n",
        "    plt.plot(history.history['val_loss'], label='Val Loss')\n",
        "    plt.title('Loss over Epochs')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('MSE Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['mae'], label='Train MAE')\n",
        "    plt.plot(history.history['val_mae'], label='Val MAE')\n",
        "    plt.title('MAE over Epochs')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('MAE')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_training_history(history)"
      ],
      "metadata": {
        "id": "HEe26Cwl3rqX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "with tqdm(total=len(X_test), desc=\"Prediction Progress\") as pbar:\n",
        "    y_pred = model.predict({'coord_input': X_test, 'user_input': u_test, 'time_input': t_test})\n",
        "    pbar.update(len(X_test))\n",
        "\n",
        "y_true_denorm = scaler.inverse_transform(\n",
        "    np.concatenate([y_test, np.zeros((len(y_test), 1))], axis=1)\n",
        ")[:, :2]\n",
        "y_pred_denorm = scaler.inverse_transform(\n",
        "    np.concatenate([y_pred, np.zeros((len(y_pred), 1))], axis=1)\n",
        ")[:, :2]\n",
        "\n",
        "evaluate_metrics(y_true_denorm, y_pred_denorm)"
      ],
      "metadata": {
        "id": "mGwS53BzGw4R"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
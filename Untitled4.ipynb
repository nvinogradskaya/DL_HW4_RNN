{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V28",
      "authorship_tag": "ABX9TyNO/uxmnCBi8KH+Y8rnt20E",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nvinogradskaya/DL_HW4_RNN/blob/main/Untitled4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install h3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ki4vshRPYpPc",
        "outputId": "d81b5350-31c0-41ed-b0d9-a31f4b88a2e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting h3\n",
            "  Downloading h3-4.2.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
            "Downloading h3-4.2.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: h3\n",
            "Successfully installed h3-4.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TKqnJS4NuiBC",
        "outputId": "735e997e-d727-4fc9-eaab-9bc87c76f2c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow\n",
            "  Downloading tensorflow-2.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Collecting astunparse>=1.6.0 (from tensorflow)\n",
            "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting flatbuffers>=24.3.25 (from tensorflow)\n",
            "  Downloading flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Collecting google-pasta>=0.1.1 (from tensorflow)\n",
            "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
            "Collecting libclang>=13.0.0 (from tensorflow)\n",
            "  Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.13.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.71.0)\n",
            "Collecting tensorboard~=2.19.0 (from tensorflow)\n",
            "  Downloading tensorboard-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.5.1)\n",
            "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow)\n",
            "  Downloading tensorflow_io_gcs_filesystem-0.37.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
            "Collecting wheel<1.0,>=0.23.0 (from astunparse>=1.6.0->tensorflow)\n",
            "  Downloading wheel-0.45.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (14.0.0)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.9)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.15.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.4.26)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/lib/python3/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.3.6)\n",
            "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard~=2.19.0->tensorflow)\n",
            "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
            "Collecting werkzeug>=1.0.1 (from tensorboard~=2.19.0->tensorflow)\n",
            "  Downloading werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Downloading tensorflow-2.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (644.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m644.9/644.9 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
            "Downloading flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
            "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m82.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.19.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m116.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_io_gcs_filesystem-0.37.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m112.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m100.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wheel-0.45.1-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: libclang, flatbuffers, wheel, werkzeug, tensorflow-io-gcs-filesystem, tensorboard-data-server, google-pasta, tensorboard, astunparse, tensorflow\n",
            "Successfully installed astunparse-1.6.3 flatbuffers-25.2.10 google-pasta-0.2.0 libclang-18.1.1 tensorboard-2.19.0 tensorboard-data-server-0.7.2 tensorflow-2.19.0 tensorflow-io-gcs-filesystem-0.37.1 werkzeug-3.1.3 wheel-0.45.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_d0AEijdSv40",
        "outputId": "13649ad4-49f2-49db-a926-5b408727f4ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import uuid\n",
        "import shutil\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "import h3\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import (Input, Dense, Concatenate, Dropout, LayerNormalization,\n",
        "                                     LSTM, Add, MultiHeadAttention, GlobalAveragePooling1D, RepeatVector)\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_USERS = 3\n",
        "SEQ_LENGTH = 10\n",
        "PRED_LENGTH = 10\n",
        "EMBEDDING_DIM = 16\n",
        "HIDDEN_DIM = 64\n",
        "BATCH_SIZE = 128\n",
        "EPOCHS = 10\n",
        "DATA_PATH = \"/content/drive/My Drive/Colab Notebooks/Data/\"\n",
        "SAVE_PATH = \"/content/drive/My Drive/Colab Notebooks/my-model-v6/\"\n",
        "SEQ_SAVE_PATH = os.path.join(SAVE_PATH, 'sequences/')\n",
        "os.makedirs(SAVE_PATH, exist_ok=True)\n",
        "os.makedirs(SEQ_SAVE_PATH, exist_ok=True)\n",
        "features = ['lat', 'lon', 'alt', 'hour_sin', 'hour_cos', 'day_sin', 'day_cos']"
      ],
      "metadata": {
        "id": "zUyJIoAZXJVO"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------- H3 ----------------------\n",
        "def latlon_to_h3(lat, lon, resolution):\n",
        "    return h3.latlng_to_cell(lat, lon, resolution)\n",
        "\n",
        "def add_h3_indices(df):\n",
        "    df['h3_500m'] = df.apply(lambda row: latlon_to_h3(row['lat'], row['lon'], 8), axis=1)  # Уровень 8 (~500м)\n",
        "    df['h3_5m'] = df.apply(lambda row: latlon_to_h3(row['lat'], row['lon'], 14), axis=1)   # Уровень 14 (~5м)\n",
        "    return df"
      ],
      "metadata": {
        "id": "oMXzhml4XWJ1"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_preprocess_data(data_path, max_users=MAX_USERS):\n",
        "    # Загрузка сырых данных\n",
        "    data = []\n",
        "    user_dirs = sorted(os.listdir(data_path))[:max_users]\n",
        "\n",
        "    for user in user_dirs:\n",
        "        traj_dir = os.path.join(data_path, user, 'Trajectory')\n",
        "        traj_files = sorted([f for f in os.listdir(traj_dir) if f.endswith('.plt')])\n",
        "\n",
        "        for traj_file in traj_files:\n",
        "            df = pd.read_csv(\n",
        "                os.path.join(traj_dir, traj_file),\n",
        "                skiprows=6, header=None, usecols=[0, 1, 3, 5, 6],\n",
        "                names=['lat', 'lon', 'alt', 'date', 'time']\n",
        "            )\n",
        "            df['user'] = user\n",
        "            data.append(df)\n",
        "\n",
        "    # Объединение и предобработка\n",
        "    df = pd.concat(data, ignore_index=True)\n",
        "\n",
        "    # Обработка времени\n",
        "    df['datetime'] = pd.to_datetime(df['date'] + ' ' + df['time'])\n",
        "    df.sort_values(by=['user', 'datetime'], inplace=True)\n",
        "\n",
        "    # Фильтрация данных\n",
        "    df = df[(df['lat'] != 0) & (df['lon'] != 0)].ffill()\n",
        "\n",
        "    # Добавление H3 индексов\n",
        "    df = add_h3_indices(df)  # Уровни 8 и 14\n",
        "\n",
        "    # Нормализация координат относительно H3 ячеек\n",
        "    def normalize_coords(row, resolution):\n",
        "    # Добавляем 'm' к resolution только в имени столбца\n",
        "        cell_center = h3.cell_to_latlng(row[f'h3_{resolution}m'])  # Теперь resolution передаётся без 'm'\n",
        "        local_lat = row['lat'] - cell_center[0]\n",
        "        local_lon = row['lon'] - cell_center[1]\n",
        "        return pd.Series([local_lat, local_lon])\n",
        "\n",
        "    # Было:\n",
        "    # Стало:\n",
        "    df[['local_lat_500m', 'local_lon_500m']] = df.apply(\n",
        "        normalize_coords, args=('500',), axis=1  # Убрали 'm' из аргумента\n",
        "    )\n",
        "    df[['local_lat_5m', 'local_lon_5m']] = df.apply(\n",
        "        normalize_coords, args=('5',), axis=1    # Убрали 'm' из аргумента\n",
        "    )\n",
        "\n",
        "    # Скалирование основных признаков\n",
        "    scaler = StandardScaler()\n",
        "    df[['lat', 'lon', 'alt']] = scaler.fit_transform(df[['lat', 'lon', 'alt']])\n",
        "\n",
        "    # Временные признаки\n",
        "    df['hour_sin'] = np.sin(2 * np.pi * df['datetime'].dt.hour / 24)\n",
        "    df['hour_cos'] = np.cos(2 * np.pi * df['datetime'].dt.hour / 24)\n",
        "    df['day_sin'] = np.sin(2 * np.pi * df['datetime'].dt.dayofweek / 7)\n",
        "    df['day_cos'] = np.cos(2 * np.pi * df['datetime'].dt.dayofweek / 7)\n",
        "\n",
        "    # Кодирование H3 индексов для макромодели\n",
        "    le_500 = LabelEncoder()\n",
        "    df['h3_500m_encoded'] = le_500.fit_transform(df['h3_500m'])\n",
        "\n",
        "    # Кодирование пользователей\n",
        "    user_ids = {user: idx for idx, user in enumerate(df['user'].unique())}\n",
        "    df['user_id'] = df['user'].map(user_ids)\n",
        "\n",
        "    return df, user_ids, scaler, le_500"
      ],
      "metadata": {
        "id": "AuP70P64XZlL"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_sequences_and_save(df, user_ids, seq_length, pred_length=PRED_LENGTH, test_size=0.3, save_path='./seq_data'):\n",
        "    os.makedirs(save_path, exist_ok=True)\n",
        "    features = ['lat', 'lon', 'alt', 'hour_sin', 'hour_cos', 'day_sin', 'day_cos']\n",
        "    targets = ['lat', 'lon']\n",
        "\n",
        "    for user, user_df in df.groupby('user'):\n",
        "        uid = user_ids[user]\n",
        "        user_df = user_df.reset_index(drop=True)\n",
        "        split_idx = int(len(user_df) * (1 - test_size))\n",
        "        if split_idx <= seq_length:\n",
        "            continue\n",
        "\n",
        "        def save_chunk(X, y, h3, is_train):\n",
        "            suffix = 'train' if is_train else 'test'\n",
        "            chunk_id = uuid.uuid4().hex\n",
        "            np.savez_compressed(\n",
        "                os.path.join(save_path, f'user_{uid}_{suffix}_{chunk_id}.npz'),\n",
        "                X=X, y=y, h3=h3, user_id=uid\n",
        "            )\n",
        "\n",
        "        def process_chunk(data, is_train=True):\n",
        "            data_values = data[features].values\n",
        "            target_values = data[targets].values\n",
        "            # Исправлено: берем последний H3-индекс в последовательности\n",
        "            h3_labels = data['h3_500m_encoded'].values[-1]  # Форма (1,)\n",
        "\n",
        "            # Создание последовательностей для фичей\n",
        "            X = np.lib.stride_tricks.sliding_window_view(data_values, (seq_length, data_values.shape[1]))\n",
        "            X = X.squeeze()\n",
        "\n",
        "            # Создание последовательностей для целей\n",
        "            y = np.lib.stride_tricks.sliding_window_view(target_values, (pred_length, target_values.shape[1]))\n",
        "            y = y.squeeze()\n",
        "\n",
        "            # Сохранение чанков\n",
        "            min_length = min(len(X), len(y))\n",
        "            X = X[:min_length]\n",
        "            y = y[:min_length]\n",
        "            h3_labels = np.full(min_length, h3_labels)  # Форма (min_length,)\n",
        "\n",
        "            for i in range(0, len(X), 1000):\n",
        "                save_chunk(X[i:i+1000], y[i:i+1000], h3_labels[i:i+1000], is_train)\n",
        "\n",
        "        process_chunk(user_df.iloc[:split_idx], True)\n",
        "        process_chunk(user_df.iloc[split_idx-seq_length:], False)"
      ],
      "metadata": {
        "id": "MF2PagXbXct9"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_all_sequences_from_disk(save_path):\n",
        "    X_train, X_test, y_train, y_test = [], [], [], []\n",
        "    h3_train, h3_test = [], []  # Добавлено для H3 меток\n",
        "    users_train, users_test = [], []\n",
        "\n",
        "    for fname in sorted(os.listdir(save_path)):\n",
        "        if not fname.endswith('.npz'):\n",
        "            continue\n",
        "\n",
        "        data = np.load(os.path.join(save_path, fname))\n",
        "        split_type = 'train' if 'train' in fname else 'test'\n",
        "        uid = int(fname.split('_')[1])\n",
        "\n",
        "        X, y = data['X'], data['y']\n",
        "        h3 = data['h3']  # Добавлена загрузка H3 меток\n",
        "\n",
        "        if split_type == 'train':\n",
        "            X_train.append(X)\n",
        "            y_train.append(y)\n",
        "            h3_train.append(h3)  # Добавлено\n",
        "            users_train.append(np.full(len(X), uid))\n",
        "        else:\n",
        "            X_test.append(X)\n",
        "            y_test.append(y)\n",
        "            h3_test.append(h3)  # Добавлено\n",
        "            users_test.append(np.full(len(X), uid))\n",
        "\n",
        "    return (\n",
        "        np.concatenate(X_train), np.concatenate(X_test),\n",
        "        np.concatenate(y_train), np.concatenate(y_test),\n",
        "        np.concatenate(h3_train), np.concatenate(h3_test),  # Добавлено\n",
        "        np.concatenate(users_train), np.concatenate(users_test)\n",
        "    )"
      ],
      "metadata": {
        "id": "4TXpRTSVXgPA"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------- Контрастивная модель ----------------------\n",
        "def contrastive_model(input_shape, embedding_dim):\n",
        "    inp = Input(shape=input_shape)\n",
        "    x = LSTM(32)(inp)\n",
        "    x = Dense(embedding_dim)(x)\n",
        "    return Model(inputs=inp, outputs=x)\n",
        "\n",
        "def triplet_loss_fn(a, p, n, margin=1.0):\n",
        "    ap_dist = tf.reduce_sum(tf.square(a - p), axis=1)\n",
        "    an_dist = tf.reduce_sum(tf.square(a - n), axis=1)\n",
        "    return tf.reduce_mean(tf.maximum(ap_dist - an_dist + margin, 0.0))\n",
        "\n",
        "def create_triplets(X, user_ids):\n",
        "    anchors, positives, negatives = [], [], []\n",
        "    for uid in np.unique(user_ids):\n",
        "        same_user_idx = np.where(user_ids == uid)[0]\n",
        "        diff_user_idx = np.where(user_ids != uid)[0]\n",
        "        if len(same_user_idx) < 2:\n",
        "            continue\n",
        "        for i in range(min(len(same_user_idx) - 1, 100)):\n",
        "            a_idx, p_idx = same_user_idx[i], same_user_idx[i+1]\n",
        "            n_idx = np.random.choice(diff_user_idx)\n",
        "            anchors.append(X[a_idx])\n",
        "            positives.append(X[p_idx])\n",
        "            negatives.append(X[n_idx])\n",
        "    return np.array(anchors), np.array(positives), np.array(negatives)\n"
      ],
      "metadata": {
        "id": "HC3-SlkwXjUR"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------- Двухуровневая модель ----------------------\n",
        "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
        "    x = MultiHeadAttention(num_heads=num_heads, key_dim=head_size, dropout=dropout)(inputs, inputs)\n",
        "    x = Add()([x, inputs])\n",
        "    x = LayerNormalization(epsilon=1e-6)(x)\n",
        "    ff = Dense(ff_dim, activation='relu')(x)\n",
        "    ff = Dense(inputs.shape[-1])(ff)\n",
        "    x = Add()([x, ff])\n",
        "    x = LayerNormalization(epsilon=1e-6)(x)\n",
        "    return x\n",
        "\n",
        "# ---------------------- Макромодель (предсказывает следующую ячейку 500м) ----------------------\n",
        "# ---------------------- Макромодель (предсказание H3-ячейки 500м) ----------------------\n",
        "def build_macro_model(input_shape, num_classes):\n",
        "    seq_input = Input(shape=input_shape)\n",
        "    x = LSTM(64)(seq_input)\n",
        "    output = Dense(num_classes, activation='softmax', name='macro_output')(x)\n",
        "    return Model(inputs=seq_input, outputs=output, name='macro_model')  # <---\n",
        "\n",
        "# ---------------------- Новая микромодель (seq2seq с вниманием) ----------------------\n",
        "def build_micro_model(input_shape, num_macro_features):\n",
        "    # Энкодер\n",
        "    encoder_inputs = Input(shape=input_shape)\n",
        "    encoder_lstm = LSTM(HIDDEN_DIM, return_sequences=True, return_state=True)\n",
        "    encoder_outputs, state_h, state_c = encoder_lstm(encoder_inputs)\n",
        "\n",
        "    # Механизм внимания\n",
        "    attention = MultiHeadAttention(num_heads=2, key_dim=HIDDEN_DIM)\n",
        "    context_vector = attention(encoder_outputs, encoder_outputs)\n",
        "\n",
        "    # Декодер с объединением макропризнаков\n",
        "    macro_features = Input(shape=(num_macro_features,))\n",
        "    decoder_input = Concatenate()([context_vector[:, -1, :], macro_features])\n",
        "    decoder_input = RepeatVector(input_shape[0])(decoder_input)  # Для последовательности\n",
        "\n",
        "    decoder_lstm = LSTM(HIDDEN_DIM, return_sequences=True)\n",
        "    decoder_outputs = decoder_lstm(decoder_input, initial_state=[state_h, state_c])\n",
        "\n",
        "    # Выходной слой\n",
        "    output = Dense(2, activation='linear', name='micro_output')(decoder_outputs)  # <- Явное имя\n",
        "    return Model(inputs=[encoder_inputs, macro_features], outputs=output, name='micro_model')\n",
        "\n",
        "\n",
        "# ---------------------- Модифицированная объединенная модель ----------------------\n",
        "# ---------------------- Кастомные слои для оберток ----------------------\n",
        "class ArgMaxLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, axis=-1, **kwargs):\n",
        "        super(ArgMaxLayer, self).__init__(**kwargs)\n",
        "        self.axis = axis\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return tf.argmax(inputs, axis=self.axis)\n",
        "\n",
        "# ---------------------- Кастомный слой для преобразования H3-идентификаторов в координаты ----------------------\n",
        "class H3CellToCoordLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, cell_centers, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.cell_centers = np.array(cell_centers, dtype=np.float32)  # Сохраняем как массив\n",
        "\n",
        "    def call(self, inputs):\n",
        "        indices = tf.cast(inputs, tf.int32)\n",
        "        return tf.gather(self.cell_centers, indices)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\"cell_centers\": self.cell_centers.tolist()})  # Сериализуемый формат\n",
        "        return config\n",
        "\n",
        "# ---------------------- Модифицированная объединенная модель ----------------------\n",
        "def build_dual_scale_model(macro_input_shape, micro_input_shape, num_macro_classes, embedding_dim, le_500):\n",
        "    # Входы\n",
        "    macro_input = Input(shape=macro_input_shape, name='macro_input')\n",
        "    micro_input = Input(shape=micro_input_shape, name='micro_input')\n",
        "    user_input = Input(shape=(embedding_dim,), name='user_input')\n",
        "\n",
        "    # Макромодель\n",
        "    macro_model = build_macro_model(macro_input_shape, num_macro_classes)\n",
        "    macro_output_raw = macro_model(macro_input)\n",
        "    macro_output = tf.keras.layers.Lambda(lambda x: x, name='macro_output')(macro_output_raw)\n",
        "\n",
        "    # Аргмакс -> координаты\n",
        "    cell_ids = le_500.classes_\n",
        "    cell_centers = [h3.cell_to_latlng(cell) for cell in cell_ids]  # Получаем координаты\n",
        "\n",
        "    # Создаем слой с центрами ячеек\n",
        "    cell_centers_layer = H3CellToCoordLayer(cell_centers)\n",
        "\n",
        "    # Вход для макромодели должен быть числовым (индексы классов)\n",
        "    macro_output_idx = ArgMaxLayer(axis=-1, name='macro_output_idx')(macro_output_raw)\n",
        "    cell_centers = cell_centers_layer(macro_output_idx)  # Используем индексы вместо строк\n",
        "\n",
        "    # Комбинируем признаки\n",
        "    combined_features = Concatenate()([cell_centers, user_input])\n",
        "\n",
        "    # Микромодель\n",
        "    micro_model = build_micro_model(micro_input_shape, num_macro_features=combined_features.shape[-1])\n",
        "    micro_output_raw = micro_model([micro_input, combined_features])\n",
        "    micro_output = tf.keras.layers.Lambda(lambda x: x, name='micro_output')(micro_output_raw)  # Также вызываем модель корректно\n",
        "\n",
        "    # Финальная модель\n",
        "    return Model(\n",
        "        inputs=[macro_input, micro_input, user_input],\n",
        "        outputs=[macro_output, micro_output],\n",
        "        name='DualScaleModel'\n",
        "    )\n"
      ],
      "metadata": {
        "id": "tcoOKwosYI_z"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CombinedDataGenerator(tf.keras.utils.Sequence):\n",
        "    def __init__(self, X_macro, X_micro, user_embeddings, y, h3_labels, batch_size=128):\n",
        "        self.X_macro = X_macro\n",
        "        self.X_micro = X_micro\n",
        "        self.user_embeddings = user_embeddings\n",
        "        self.y = y\n",
        "        self.h3_labels = h3_labels\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def __len__(self):\n",
        "        # Количество батчей на эпоху\n",
        "        return int(np.ceil(len(self.X_macro) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        batch_X_macro = self.X_macro[index*self.batch_size : (index+1)*self.batch_size]\n",
        "        batch_X_micro = self.X_micro[index*self.batch_size : (index+1)*self.batch_size]\n",
        "        batch_user = self.user_embeddings[index*self.batch_size : (index+1)*self.batch_size]\n",
        "        batch_y = self.y[index*self.batch_size : (index+1)*self.batch_size]\n",
        "        batch_h3 = self.h3_labels[index*self.batch_size : (index+1)*self.batch_size]  # Форма (batch_size,)\n",
        "\n",
        "        return (\n",
        "            {'macro_input': batch_X_macro, 'micro_input': batch_X_micro, 'user_input': batch_user},\n",
        "            {'macro_output': batch_h3, 'micro_output': batch_y}  # Теперь batch_h3 имеет форму (batch_size,)\n",
        "        )"
      ],
      "metadata": {
        "id": "L3pmi0S9YNJv"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------- Метрики ----------------------\n",
        "def evaluate_model(model, X_test, user_embeddings_test, y_test):\n",
        "    macro_pred, micro_pred = model.predict([X_test, user_embeddings_test], batch_size=128)\n",
        "\n",
        "    # ADE: средняя ошибка по всем предсказанным точкам\n",
        "    ade = np.mean(np.linalg.norm(micro_pred - y_test, axis=1))\n",
        "\n",
        "    # FDE: ошибка только для последней точки\n",
        "    fde = np.mean(np.linalg.norm(micro_pred[:, -1] - y_test[:, -1], axis=1))  # <- Исправлено\n",
        "\n",
        "    dist = np.linalg.norm(micro_pred - y_test, axis=1)\n",
        "    within_100m = np.mean(dist < 0.01) * 100\n",
        "    print(f\"ADE: {ade:.4f} | FDE: {fde:.4f} | <100m: {within_100m:.2f}%\")\n",
        "    return ade, fde, within_100m"
      ],
      "metadata": {
        "id": "dw4tegf0YP5x"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------- Запуск pipeline ----------------------\n",
        "df, user_ids, scaler, le_500 = load_and_preprocess_data(DATA_PATH)"
      ],
      "metadata": {
        "id": "2VXfBfcyYTDC"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shutil.rmtree(SEQ_SAVE_PATH, ignore_errors=True)\n",
        "os.makedirs(SEQ_SAVE_PATH, exist_ok=True)\n",
        "create_sequences_and_save(df, user_ids, SEQ_LENGTH, save_path=SEQ_SAVE_PATH)"
      ],
      "metadata": {
        "id": "HlWu9jczYelr"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test, h3_train, h3_test, users_train, users_test = load_all_sequences_from_disk(SEQ_SAVE_PATH)\n",
        "print(\"Проверка размерностей:\")\n",
        "print(f\"X_train: {X_train.shape}\")  # (примеры, SEQ_LENGTH, признаки)\n",
        "print(f\"y_train: {y_train.shape}\")"
      ],
      "metadata": {
        "id": "TX0yLsS_Yj0R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbf73b6d-2178-44d9-9d67-8ee615e28b51"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Проверка размерностей:\n",
            "X_train: (371456, 10, 7)\n",
            "y_train: (371456, 10, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Проверьте формы\n",
        "print(f\"h3_train shape: {h3_train.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "io5DKMfdN6cH",
        "outputId": "8421f9b5-ca94-49fc-ab06-fe6e071cc841"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "h3_train shape: (371456,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"X_train: {X_train.shape}, h3_train: {h3_train.shape}\")  # Должны совпадать по первому измерению"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "--SZSDA4L4TS",
        "outputId": "f124c8eb-e762-4089-a4a3-4f8c080a239e"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train: (371456, 10, 7), h3_train: (371456,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "anchors, positives, negatives = create_triplets(X_train, users_train)\n",
        "triplet_encoder = contrastive_model(X_train.shape[1:], EMBEDDING_DIM)\n",
        "optimizer = tf.keras.optimizers.Adam(1e-3)"
      ],
      "metadata": {
        "id": "CaNI-rAaYkdp"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Исправленный блок обучения с EarlyStopping (в памяти)\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor=\"loss\",\n",
        "    patience=2,\n",
        "    restore_best_weights=True,\n",
        "    mode=\"min\"\n",
        ")\n",
        "\n",
        "best_weights = None\n",
        "best_loss = float('inf')\n",
        "wait = 0\n",
        "\n",
        "for epoch in range(15):\n",
        "    with tf.GradientTape() as tape:\n",
        "        emb_a = triplet_encoder(anchors)\n",
        "        emb_p = triplet_encoder(positives)\n",
        "        emb_n = triplet_encoder(negatives)\n",
        "        loss = triplet_loss_fn(emb_a, emb_p, emb_n)\n",
        "    grads = tape.gradient(loss, triplet_encoder.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, triplet_encoder.trainable_variables))\n",
        "\n",
        "    print(f\"contrastive epoch {epoch+1} // loss = {loss.numpy():.4f}\")\n",
        "\n",
        "    if loss < best_loss:\n",
        "        best_loss = loss\n",
        "        best_weights = triplet_encoder.get_weights()\n",
        "        wait = 0\n",
        "    else:\n",
        "        wait += 1\n",
        "        if wait >= early_stopping.patience:\n",
        "            print(f\"Ранняя остановка на эпохе {epoch+1}\")\n",
        "            triplet_encoder.set_weights(best_weights)\n",
        "            break\n",
        "\n",
        "if epoch + 1 < 10:\n",
        "    print(f\"Восстановлены лучшие веса с loss = {best_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l6-oA6qFYnK9",
        "outputId": "a394f011-bbe6-405e-dbfa-d33e4566c43b"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "contrastive epoch 1 // loss = 0.2218\n",
            "contrastive epoch 2 // loss = 0.2029\n",
            "contrastive epoch 3 // loss = 0.1876\n",
            "contrastive epoch 4 // loss = 0.1731\n",
            "contrastive epoch 5 // loss = 0.1592\n",
            "contrastive epoch 6 // loss = 0.1457\n",
            "contrastive epoch 7 // loss = 0.1345\n",
            "contrastive epoch 8 // loss = 0.1252\n",
            "contrastive epoch 9 // loss = 0.1165\n",
            "contrastive epoch 10 // loss = 0.1088\n",
            "contrastive epoch 11 // loss = 0.1009\n",
            "contrastive epoch 12 // loss = 0.0931\n",
            "contrastive epoch 13 // loss = 0.0855\n",
            "contrastive epoch 14 // loss = 0.0782\n",
            "contrastive epoch 15 // loss = 0.0720\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "user_embeddings_matrix = {}\n",
        "for uid in np.unique(users_train):\n",
        "    user_seqs = X_train[users_train == uid]\n",
        "    user_embs = triplet_encoder.predict(user_seqs, batch_size=1024)\n",
        "    user_embeddings_matrix[uid] = np.mean(user_embs, axis=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YR-actdOYpuu",
        "outputId": "72f15e5e-f325-468c-d096-fee3a7c13679"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step\n",
            "\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "user_embeddings_train = np.array([user_embeddings_matrix[uid] for uid in users_train])\n",
        "user_embeddings_test = np.array([user_embeddings_matrix[uid] for uid in users_test])"
      ],
      "metadata": {
        "id": "dgphkFZrYsCO"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_tf_dataset(generator):\n",
        "    output_signature = (\n",
        "        {\n",
        "            'macro_input': tf.TensorSpec(shape=(None, SEQ_LENGTH, len(features)), dtype=tf.float32),\n",
        "            'micro_input': tf.TensorSpec(shape=(None, SEQ_LENGTH, len(features)), dtype=tf.float32),\n",
        "            'user_input': tf.TensorSpec(shape=(None, EMBEDDING_DIM), dtype=tf.float32)\n",
        "        },\n",
        "        {\n",
        "            'macro_output': tf.TensorSpec(shape=(None,), dtype=tf.int32),\n",
        "            'micro_output': tf.TensorSpec(shape=(None, PRED_LENGTH, 2), dtype=tf.float32)\n",
        "        }\n",
        "    )\n",
        "    return tf.data.Dataset.from_generator(\n",
        "        lambda: generator,\n",
        "        output_signature=output_signature\n",
        "    )"
      ],
      "metadata": {
        "id": "qWcmQI9FJcSK"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_dual_scale_model(\n",
        "    macro_input_shape=(SEQ_LENGTH, len(features)),\n",
        "    micro_input_shape=(SEQ_LENGTH, len(features)),\n",
        "    num_macro_classes=len(le_500.classes_),\n",
        "    embedding_dim=EMBEDDING_DIM,\n",
        "    le_500=le_500\n",
        ")\n",
        "\n",
        "# Компиляция\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss={\n",
        "        'macro_output': 'sparse_categorical_crossentropy',\n",
        "        'micro_output': 'mse'\n",
        "    },\n",
        "    loss_weights={'macro_output': 0.3, 'micro_output': 0.7},\n",
        "    metrics={'micro_output': ['mae']}\n",
        ")"
      ],
      "metadata": {
        "id": "SNci6X1vYuFM"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.output_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fZX8Z1LSLZ_J",
        "outputId": "3dbf4872-2d69-43a8-f9a8-aa50366766ab"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ListWrapper(['macro_output', 'micro_output'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_gen = CombinedDataGenerator(X_train, X_train, user_embeddings_train, y_train, h3_train, BATCH_SIZE)\n",
        "val_gen = CombinedDataGenerator(X_test, X_test, user_embeddings_test, y_test, h3_test, BATCH_SIZE)"
      ],
      "metadata": {
        "id": "mLwXMnoxJfrn"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = create_tf_dataset(train_gen)\n",
        "val_dataset = create_tf_dataset(val_gen)"
      ],
      "metadata": {
        "id": "SCf7Cm7AJgum"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SAVE_PATH2 = \"/content/drive/My Drive/Colab Notebooks/my-model-v3/model.keras\"\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=val_dataset,\n",
        "    epochs=5,\n",
        "    steps_per_epoch=len(train_gen),  # Количество шагов в эпохе\n",
        "    validation_steps=len(val_gen),\n",
        "    callbacks=[\n",
        "        EarlyStopping(patience=3, restore_best_weights=True),\n",
        "        ModelCheckpoint(SAVE_PATH2, save_best_only=True)\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NggAoA9yJl1l",
        "outputId": "5a83e245-f23f-4963-dc8c-e9e28b71ad76"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m2902/2902\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 34ms/step - loss: 0.3886 - macro_output_loss: 0.7580 - micro_output_loss: 0.2303 - micro_output_mae: 0.1306 - val_loss: 3.9851 - val_macro_output_loss: 13.1779 - val_micro_output_loss: 0.0454 - val_micro_output_mae: 0.0855\n",
            "Epoch 2/5\n",
            "\u001b[1m2902/2902\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 11ms/step - loss: 0.0000e+00 - macro_output_loss: 0.0000e+00 - micro_output_loss: 0.0000e+00 - micro_output_mae: 0.0000e+00 - val_loss: 3.9883 - val_macro_output_loss: 13.1866 - val_micro_output_loss: 0.0463 - val_micro_output_mae: 0.0941\n",
            "Epoch 3/5\n",
            "\u001b[1m2902/2902\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 11ms/step - loss: 0.0000e+00 - macro_output_loss: 0.0000e+00 - micro_output_loss: 0.0000e+00 - micro_output_mae: 0.0000e+00 - val_loss: 3.9883 - val_macro_output_loss: 13.1866 - val_micro_output_loss: 0.0463 - val_micro_output_mae: 0.0941\n",
            "Epoch 4/5\n",
            "\u001b[1m2902/2902\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 12ms/step - loss: 0.0000e+00 - macro_output_loss: 0.0000e+00 - micro_output_loss: 0.0000e+00 - micro_output_mae: 0.0000e+00 - val_loss: 3.9883 - val_macro_output_loss: 13.1866 - val_micro_output_loss: 0.0463 - val_micro_output_mae: 0.0941\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, X_test, user_embeddings_test, y_test, le_500):\n",
        "    # Предсказание\n",
        "    macro_pred, micro_pred = model.predict([X_test, user_embeddings_test])\n",
        "\n",
        "    # Преобразование локальных координат в глобальные\n",
        "    cell_ids = np.argmax(macro_pred, axis=1)\n",
        "    cell_centers = le_500.inverse_transform(cell_ids)\n",
        "\n",
        "    # Расчет глобальных координат\n",
        "    global_pred = cell_centers[:, None, :] + micro_pred\n",
        "\n",
        "    # Расчет метрик\n",
        "    ade = np.mean(np.linalg.norm(global_pred - y_test_global, axis=2))\n",
        "    fde = np.mean(np.linalg.norm(global_pred[:, -1, :] - y_test_global[:, -1, :], axis=1))\n",
        "\n",
        "    print(f\"ADE: {ade:.2f} м | FDE: {fde:.2f} м\")\n",
        "    return ade, fde"
      ],
      "metadata": {
        "id": "Kzm_hEonv4qf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
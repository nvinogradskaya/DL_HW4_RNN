{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO/Skp/MT3vDc2ZpqMoSlmq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nvinogradskaya/DL_HW4_RNN/blob/main/geolifev2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Z2uFmr0gYJOx"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, Dropout, Embedding, Lambda, Concatenate, RepeatVector\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')\n",
        "\n",
        "DATA_PATH = \"/content/drive/My Drive/Colab Notebooks/Data/\"\n",
        "SEQ_LENGTH = 10\n",
        "EMBEDDING_DIM = 16\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MsrMi0COYdpC",
        "outputId": "b8e04780-5b1c-42a8-b55e-a0b523ed250b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_preprocess_data(data_path, max_users=10):\n",
        "    data = []\n",
        "    user_dirs = sorted(os.listdir(data_path))[:max_users]\n",
        "\n",
        "    for user in user_dirs:\n",
        "        traj_dir = os.path.join(data_path, user, 'Trajectory')\n",
        "        traj_files = [f for f in os.listdir(traj_dir) if f.endswith('.plt')]\n",
        "\n",
        "        for traj_file in traj_files:\n",
        "            df = pd.read_csv(\n",
        "                os.path.join(traj_dir, traj_file),\n",
        "                skiprows=6,\n",
        "                header=None,\n",
        "                usecols=[0, 1, 3, 5, 6],\n",
        "                names=['lat', 'lon', 'alt', 'date', 'time']\n",
        "            )\n",
        "            df['user'] = user\n",
        "            data.append(df)\n",
        "\n",
        "    df = pd.concat(data, ignore_index=True)\n",
        "    df['datetime'] = pd.to_datetime(df['date'] + ' ' + df['time'])\n",
        "    df.sort_values(by=['user', 'datetime'], inplace=True)\n",
        "\n",
        "    # Фильтрация и нормализация\n",
        "    df = df[(df['lat'] != 0) & (df['lon'] != 0)].ffill()\n",
        "    scaler = MinMaxScaler()\n",
        "    df[['lat', 'lon', 'alt']] = scaler.fit_transform(df[['lat', 'lon', 'alt']])\n",
        "\n",
        "    # Временные признаки\n",
        "    df['hour_sin'] = np.sin(2 * np.pi * df['datetime'].dt.hour / 24)\n",
        "    df['hour_cos'] = np.cos(2 * np.pi * df['datetime'].dt.hour / 24)\n",
        "    df['day_sin'] = np.sin(2 * np.pi * df['datetime'].dt.dayofweek / 7)\n",
        "    df['day_cos'] = np.cos(2 * np.pi * df['datetime'].dt.dayofweek / 7)\n",
        "\n",
        "    # Персонализированные эмбеддинги\n",
        "    user_ids = {user: idx for idx, user in enumerate(df['user'].unique())}\n",
        "    df['user_id'] = df['user'].map(user_ids)\n",
        "\n",
        "    return df, user_ids, scaler"
      ],
      "metadata": {
        "id": "j6E0283MYsuO"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_sequences(df, user_ids, seq_length):\n",
        "    features = ['lat', 'lon', 'alt', 'hour_sin', 'hour_cos', 'day_sin', 'day_cos']\n",
        "    targets = ['lat', 'lon']\n",
        "\n",
        "    X, y, users = [], [], []\n",
        "    for user, group in df.groupby('user'):\n",
        "        user_data = group[features].values\n",
        "        user_targets = group[targets].values\n",
        "        user_id = user_ids[user]\n",
        "\n",
        "        for i in range(len(user_data) - seq_length):\n",
        "            X.append(user_data[i:i+seq_length])\n",
        "            y.append(user_targets[i+seq_length])\n",
        "            users.append(user_id)\n",
        "\n",
        "    return np.array(X), np.array(y), np.array(users)"
      ],
      "metadata": {
        "id": "FQJPGk3ZY27p"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ContrastiveLoss(tf.keras.losses.Loss):\n",
        "    def __init__(self, margin=1.0):\n",
        "        super().__init__()\n",
        "        self.margin = margin\n",
        "\n",
        "    def call(self, y_true, y_pred):\n",
        "        y_true = tf.cast(y_true, tf.float32)\n",
        "        anchor, positive = y_pred[:,0], y_pred[:,1]\n",
        "        distances = tf.reduce_sum(tf.square(anchor - positive), axis=-1)\n",
        "        loss = y_true * distances + (1 - y_true) * tf.maximum(self.margin - distances, 0)\n",
        "        return tf.reduce_mean(loss)"
      ],
      "metadata": {
        "id": "hiQow8wKZOM4"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(seq_length, num_features, num_users, embedding_dim):\n",
        "    # Входные слои\n",
        "    user_input = Input(shape=(1,), name='user_input')\n",
        "    traj_input = Input(shape=(seq_length, num_features), name='traj_input')\n",
        "\n",
        "    # Персонализированные эмбеддинги\n",
        "    user_embedding = Embedding(num_users, embedding_dim)(user_input)\n",
        "    user_embedding = Lambda(lambda x: tf.squeeze(x, axis=1))(user_embedding)\n",
        "    user_embedding = RepeatVector(seq_length)(user_embedding)\n",
        "\n",
        "    # Объединение признаков\n",
        "    merged = Concatenate(axis=-1)([traj_input, user_embedding])\n",
        "\n",
        "    # LSTM сеть\n",
        "    x = LSTM(64, return_sequences=True)(merged)\n",
        "    x = Dropout(0.2)(x)\n",
        "    x = LSTM(32)(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "\n",
        "    # Выходной слой\n",
        "    output = Dense(2, activation='linear')(x)\n",
        "\n",
        "    return Model(inputs=[traj_input, user_input], outputs=output)"
      ],
      "metadata": {
        "id": "kGsM5R6nZTnD"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # Загрузка данных\n",
        "    df, user_ids, scaler = load_and_preprocess_data(DATA_PATH)\n",
        "\n",
        "    # Создание последовательностей\n",
        "    X, y, users = create_sequences(df, user_ids, SEQ_LENGTH)\n",
        "\n",
        "    # Разделение данных\n",
        "    X_train, X_test, y_train, y_test, users_train, users_test = train_test_split(\n",
        "        X, y, users, test_size=0.2, random_state=42\n",
        "    )\n",
        "\n",
        "    # Построение модели\n",
        "    model = build_model(\n",
        "        seq_length=SEQ_LENGTH,\n",
        "        num_features=X_train.shape[-1],\n",
        "        num_users=len(user_ids),\n",
        "        embedding_dim=EMBEDDING_DIM\n",
        "    )\n",
        "\n",
        "    # Компиляция модели\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=0.001),\n",
        "        loss='mse',\n",
        "        metrics=['mae']\n",
        "    )\n",
        "\n",
        "    # Обучение\n",
        "    early_stopping = EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=5,\n",
        "        restore_best_weights=True\n",
        "    )\n",
        "\n",
        "    history = model.fit(\n",
        "        [X_train, users_train],\n",
        "        y_train,\n",
        "        validation_split=0.2,\n",
        "        epochs=EPOCHS,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        callbacks=[early_stopping]\n",
        "    )\n",
        "\n",
        "    # Оценка\n",
        "    test_loss, test_mae = model.evaluate([X_test, users_test], y_test)\n",
        "    print(f\"Test Loss: {test_loss:.4f}, Test MAE: {test_mae:.4f}\")"
      ],
      "metadata": {
        "id": "aGl6Zpy_ZYdw"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hG9-H8YyZf-N",
        "outputId": "7003b12f-5761-41c7-80bf-cdf04f056fc4"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m18459/18459\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m325s\u001b[0m 17ms/step - loss: 0.0036 - mae: 0.0313 - val_loss: 4.6659e-05 - val_mae: 0.0033\n",
            "Epoch 2/5\n",
            "\u001b[1m18459/18459\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m324s\u001b[0m 17ms/step - loss: 2.2284e-04 - mae: 0.0067 - val_loss: 5.5401e-05 - val_mae: 0.0038\n",
            "Epoch 3/5\n",
            "\u001b[1m18459/18459\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m321s\u001b[0m 17ms/step - loss: 2.0788e-04 - mae: 0.0061 - val_loss: 2.7046e-05 - val_mae: 0.0030\n",
            "Epoch 4/5\n",
            "\u001b[1m18459/18459\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m319s\u001b[0m 17ms/step - loss: 2.0300e-04 - mae: 0.0059 - val_loss: 3.8089e-05 - val_mae: 0.0031\n",
            "Epoch 5/5\n",
            "\u001b[1m18459/18459\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m319s\u001b[0m 17ms/step - loss: 2.0004e-04 - mae: 0.0058 - val_loss: 3.8482e-05 - val_mae: 0.0035\n",
            "\u001b[1m11537/11537\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 5ms/step - loss: 2.7367e-05 - mae: 0.0030\n",
            "Test Loss: 0.0000, Test MAE: 0.0030\n"
          ]
        }
      ]
    }
  ]
}